---
title: "Project in regression - practical part"
output:
  pdf_document:
    toc: yes
  html_document:
    code_folding: hide
    toc: yes
---

```{r warning = FALSE, message = FALSE, echo = FALSE}
#Loading relevant libraries
knitr::opts_chunk$set(results = "hold")

library(GLMsData)     # Project specific package
library(statmod)      # Project specific package
library(tweedie)      # Project specific package
library(mgcv)         # Project specific package

library(dplyr)        # For data manipulation
library(reshape2)     # For data manipulation
library(tidyverse)    # For data manipulation

library(splines)      # For splines

library(ggplot2)      # For plotting
theme_set(theme_bw()) # Set theme for ggplot2
library(gridExtra)    # For arranging plots
library(grid)         # For arranging plots
library(lattice)      # For plotting splom()
library(hexbin)       # For plotting hexbin

library(Hmisc)        # For multiple imputation
library(mice)         # For multiple imputation
library(sjmisc)       # For combining multiple imputation

library(pander)       # For printing tables
library(knitr)        # For printing tables
```


In this project we seek to predict the rainfall at Eromanga in Queensland, Australia, during the month of July. We consider a data set that contains total rainfall in July at Eromanga in the period 1905 to 2024 with the exception of a few years. In addition we have measurements of the southern oscillation index (SOI, the standardized difference between the air pressures at Darwin and Haiti, related to el ni√±o) for the same years. The hypothesis is that the SOI is related to the rainfall in Eromanga. We will throughout the project investigate this hypothesis and ultimately use the SOI index to predict the rainfall in Eromanga.

To increase readability we have omitted code chunks on several occasions. In particular when it comes to plots and other standard code parts. When the code and output is more involved and essential for the analysis and understanding of the project we have included it in the project. If anything is unclear all code can be accessed at https://github.com/miclukacova/reg_project.

# Exploratory data analysis

First, we load the data set

```{r}
#Rain.data = read.table("~/Desktop/Studie/Master/First year/Block 1/Regression/Project/RaindataEromanga.txt", header = TRUE, colClasses = c("integer", "numeric", "integer", "numeric", "factor"))

Rain.data = read.table("RaindataEromanga.txt", header = TRUE, colClasses = c("integer", "numeric", "integer", "numeric", "factor"))
```

We specify the column classes and print the `head()` and `summary()` of the data set to make sure that the data is read in correctly.

```{r, echo = FALSE}
head(Rain.data)
summary(Rain.data)
```

The data set contains five variables: `Year`, `Rain`, `SOI`, `Phase` and `Month.` The `Month` variable is constant and will not analyzed, but keep in mind, that the analysis carried out is for the month of July. The response variable `Rain` represents the total rainfall in July at Eromanga and contains nine missing values, which will be addressed in a subsequent part of the EDA. The `SOI` variable is the southern oscillation index, while `Phase` is a categorical variable indicating the SOI phase on five different levels:

+ Phase 1: Consistently negative
+ Phase 2: Consistently positive
+ Phase 3: Rapidly falling
+ Phase 4: Rapidly rising
+ Phase 5: Consistently near zero

We plot the marginal distributions of the variables to visually explore the data. 

```{r warning = FALSE, echo = FALSE, fig.width = 5.5, fig.height = 3, fig.align = 'center'}
tmp <- lapply(names(Rain.data), function(x)
  ggplot(data = Rain.data[, x, drop = FALSE]) +
    aes_string(x) +
    xlab(x) + 
    ylab("") +
    theme_bw()
  )

gd <- geom_density(adjust = 2, fill = gray(0.5))
gb <- geom_bar(fill = (gray(0.5)))

grid.arrange(
  tmp[[2]] + gd,
  tmp[[4]] + gd,
  tmp[[5]] + gb,
  ncol = 3
)
```

We observe that the response variable `Rain` is right-skewed, which should be considered in later analysis. Additionally, Category 3 in `Phase` has relatively few observations, which may also require consideration in subsequent analysis. 

We proceed to investegate possible co-linearity between the variables in the data set. We assess correlation between the numerical variables `Rainfall`, `SOI` and `Year` with a correlation plot:

```{r warning = FALSE, echo = FALSE, fig.width = 3, fig.height = 3, fig.align = 'center'}
cor.print <- function(x, y){
  panel.text(mean(range(x)), mean(range(y)),
             paste(round(cor(x,y), digits = 2), sep = '')
             )
}
contVar <- c("Rain", "Year", "SOI")
splom(na.omit(Rain.data)[, contVar], xlab = "",
      upper.panel = panel.hexbinplot,
      pscales = 0, xbins = 20,
      varnames = contVar,
      lower.panel = cor.print)
```

`Year` is almost uncorrelated with both `Rain` and `SOI`. There is a weak positive correlation between `Rain` and `SOI`.

```{r warning = FALSE, echo = FALSE, fig.width = 3, fig.height = 3, fig.align = 'center'}
# cp <- cor(data.matrix(subset(Rain.data %>% dplyr::filter(!is.na(Rain)), select = -c(Month, Phase))), method = "spearman")
# ord <- rev(hclust(as.dist(1 - abs(cp)))$order)
# colPal <- colorRampPalette(c("blue", "yellow"), space = "rgb")(100)
# 
# levelplot(cp[ord, ord],
#           xlab = "", 
#           ylab = "",
#           col.regions = colPal, 
#           at = seq(-1, 1, length.out=100),
#           colorkey = list(space = "top", labels = list(cex = 1.5)),
#           scales = list(x = list(rot = 45),
#                         y = list(draw = FALSE),
#                         cex = 1.2)
# )
```

To investigate the relation between `Rain` and `Phase` we consider the distribution of `Rain` stratified by `Phase`:

```{r warning = FALSE, echo = FALSE, fig.width = 5, fig.height = 4, fig.align = 'center'}
facVar <- c("Phase")
mRain <- melt(Rain.data[, c("Rain", facVar)],
              id = "Rain")

ggplot(mRain,
       aes(x = factor(value, levels = 1:5), y = Rain)) +
  geom_boxplot(fill = I(gray(0.8))) + xlab("") +
  facet_wrap(~ variable, scale = "free_x", ncol = 2) + theme_bw()
```

The boxplot suggests that the rainfall is larger in phase 2 and 4, while it appears particularly low in phase 1 and 2.

We finally investigate the relation between `SOI` and `Phase`. Since both variables describe aspects of SOI we expect some correlation between them.

```{r}
summary(lm(SOI ~ Phase, Rain.data))
```
The simple linear model uncovers a high adjusted $R^2$ implying a high correlation between the two variables. All coefficients except `Phase3` are significant indicating that the `Phase` variable is a good predictor of `SOI`. This discovery can further be supported by the boxplot below.

```{r warning = FALSE, echo = FALSE, fig.width = 5, fig.height = 4, fig.align = 'center'}
facVar <- c("Phase")
mSOI <- melt(Rain.data[, c("SOI", facVar)],
              id = "SOI")

ggplot(mSOI,
       aes(x = factor(value, levels = 1:5), y = SOI)) +
  geom_boxplot(fill = I(gray(0.8))) + xlab("") +
  facet_wrap(~ variable, scale = "free_x", ncol = 2) + theme_bw()
```
Since the two predictors are collinear to some extent we will continue with care, when we consider models with both covariates. It is worth noting though, that while `SOI` is a numeric variable containing more exact values of SOI, `Phase` also contains information on the 'direction' of the SOI, i.e. whether it is increasing, decreasing or constant. So although the two variables are correlated they both contain information that the other does not so it could still be valuable to include both variables in a model. We will go into further details on this matter in subsequent sections. 


## Missing data
With a better understanding of the data we proceed to investigate and handle the missing data. As noted previously there are only missing data (7.5 pct. missing) in the response variable `Rain`.

```{r}
Rain.data %>% dplyr::filter(is.na(Rain))
```

We plot the marginal distribution of the variables with missing data to obtain a visual understanding of the missing data:

```{r warning = FALSE, echo = FALSE, fig.width = 5.5, fig.height = 3, fig.align = 'center'}
tmp_missing <- lapply(names(Rain.data), function(x)
  ggplot(data = (Rain.data %>% dplyr::filter(is.na(Rain)))[, x, drop = FALSE]) +
    aes_string(x) +
    xlab(x) + 
    ylab("") +
    theme_bw()
  )

grid.arrange(
  tmp_missing[[4]] + gd,
  tmp_missing[[5]] + gb,
  ncol = 2
)
```

Given the small number of missing observations, it is difficult to identify any clear trends in the missingness. We note that the missingness occurs in consecutive years (except 1933), but there are no distinct pattern beyond that. Without metadata and additional knowledge we are unable to determine if the data is missing completely at random (MCAR), at random (MAR) or not at random (MNAR). It seems unlikely that observations of large rainfall were selectively deleted or that someone was too lazy to record rainfall during heavy rainfalls. A more plausible explanation, given the consecutive years, is that the equipment may be broken for consecutive years or there was a lack of funding these years. The latter explanations would imply that the data is MCAR which implies MAR and we therefore choose to adopt this assumption.

Assuming MAR we can use multiple imputation techniques to impute the missing data. If the MAR assumption holds the imputed values are unbiased and the variation in the data set is preserved. We use the `mice()` function (multiple imputations using chained equations) from the package `mice` to perform the multiple imputations. This procedure use random draws from the conditional distribution of the target variable given the other variables. That is, we take a bootstrap sample from our data and fit a regression model to this sample to predict the missing values. We use _predictive mean matching_ (PMM) to replace the missing values in each imputed data set. This method uses the value of a donor observations to fill in the missing values. The donors are identified by matching the predicted value of the target to the donor value. PMM does not require any distributional assumptions and is therefore a fairly robust method (F. E. Harell, Jr., 2015). 

We run the imputations and display the first five rows of the first two imputed data sets:

```{r warning = FALSE}
Rain.data.impute <- mice(Rain.data, method = "pmm", 
                         m = 5, seed = 10102024,  
                         printFlag = FALSE)
head(complete(Rain.data.impute,1))
head(complete(Rain.data.impute,2))
```

We plot the density of the five imputed data sets (red dotted) along with the density of the original data set (black solid) to check if the imputed data sets resemble each other and the original data set.

```{r warning = FALSE, echo = FALSE, fig.width = 5, fig.height = 4, fig.align = 'center'}
ggplot(data = Rain.data, aes(x = Rain)) +
  geom_density(size = 1) +
  ggtitle("Density plot of original data and imputed data sets") +
  geom_density(data = complete(Rain.data.impute, 1), 
               aes(x = Rain), color = "red", size = 0.5, lty = 2) +
  geom_density(data = complete(Rain.data.impute, 2), 
               aes(x = Rain), color = "red", size = 0.5, lty = 2) +
  geom_density(data = complete(Rain.data.impute, 3), 
               aes(x = Rain), color = "red", size = 0.5, lty = 2) +
  geom_density(data = complete(Rain.data.impute, 4), 
               aes(x = Rain), color = "red", size = 0.5, lty = 2) +
  geom_density(data = complete(Rain.data.impute, 5), 
               aes(x = Rain), color = "red", size = 0.5, lty = 2) +
  ylab("Density") +
  theme_bw()
```

We note that the densities for all the imputed data sets look very similar to the original data set. We therefore proceed with the imputed data sets.


# Analysis using SOI phase
In the following part of the project we seek to fit the Tweedie exponential dispersion model to the data, and predict rainfall as a function of the SOI phase. In order to fit a Tweedie model to the data we need to estimate the nuisance parameter $k$ which we assume to be between 1 and 2. 


## Estimating k with a linear regression model
We initially try to estimate $k$ using the linear relation $VY = \psi \mathcal{V}(\mu) = \psi \mu^k$. This implies that $\log (VY) = \log(\psi) + k \log(\mu)$. We can therefore estimate $k$ by a linear regression of $\log (VY)$ on $\log(\mu)$. We estimate the variance and mean of the response variable within each SOI phase for each imputed data set using the empirical mean and variance:

```{r}
grouped_imputatations <- list()
for (i in 1:5) {
  grouped_imputatations[[i]] <- complete(Rain.data.impute, i) %>% group_by(Phase) %>% 
    summarise(meanY = mean(Rain), varY = var(Rain))
}
pander(grouped_imputatations)
```

With estimates of the mean and variance of $Y$ we can proceed to fit an additive linear regression to estimate $k$ and the dispersion parameter for each imputed data set:

```{r}
lm.fit.imputed <- list()
for (i in 1:5) {
  lm.fit.imputed[[i]] <- lm(log(varY) ~ log(meanY), data = grouped_imputatations[[i]])
}
pander(lm.fit.imputed)
```


We get an initial estimate of $k$ between $1.388$ and $1.470$ and $\psi$ between $\exp(1.975) = 7.201$ and $\exp(2.133) = 8.440$. Naturally the estimates vary due to the randomness in the imputations. However, like the density plots the predictions are fairly similar. A simple confidence interval for the estimates of each imputation can be plotted:

```{r warning = FALSE, echo = FALSE, fig.width = 5, fig.height = 4, fig.align = 'center'}
k_hat_lin_conf <- list()
for (i in 1:5) {
  k_hat_lin_confest <- lm.fit.imputed[[i]]$coef[[2]]
  k_hat_lin_confsd <- summary(lm.fit.imputed[[i]])$coef[2,2]
  k_hat_lin_conf[[i]] <- list("Estimate" = k_hat_lin_confest,
                           "LowerCI" = lm.fit.imputed[[i]]$coef[[2]] - 1.96 * summary(lm.fit.imputed[[i]])$coef[2,2],
                           "UpperCI" = lm.fit.imputed[[i]]$coef[[2]] + 1.96 * summary(lm.fit.imputed[[i]])$coef[2,2])
}

estimates_df <- do.call(rbind, lapply(k_hat_lin_conf, as.data.frame))
estimates_df$Imputation <- paste("Imputation", 1:nrow(estimates_df))  # Add model labels

ggplot(estimates_df, aes(x = Imputation, y = Estimate, ymin = LowerCI, ymax = UpperCI)) +
  geom_pointrange() +
  geom_errorbar(aes(ymin = LowerCI, ymax = UpperCI), width = 0.2, color = "black") +
  theme_minimal() +
  labs(title = "k estimates for each imputation with Confidence Intervals",
       x = "Imputation",
       y = "Estimate") +
  coord_flip()  # Flip coordinates for a horizontal display
```
As all values of $k$ are within a reasonable margin of error, we choose to merge the imputations into a single data set to simplify calculations and communicate results more clearly. We should however keep in mind that this will reduce the variance of the data slightly and for a more thorough analysis we would keep all five imputed data sets. 

We use the `merge_imputations` from the `sjmisc` package which merges multiple imputed data frames from `mice::mids()-objects` into a single data frame by computing the mean or selecting the most likely imputed value.

```{r}
Rain.data.comp <- Rain.data %>% 
  mutate(Rain = merge_imputations(Rain.data, Rain.data.impute)$Rain)

Rain.data.grp <- Rain.data.comp %>% 
  group_by(Phase) %>% 
  summarise(meanY = mean(Rain), varY = var(Rain))

lm.fit <- lm(log(varY) ~ log(meanY), data = Rain.data.grp)
pander(lm.fit)
```

We end up with estimates for $k$ and $\psi$ of

```{r}
k_hat_lin <- lm.fit$coef[[2]]
psi_hat_lin <- exp(lm.fit$coef[[1]])

pander(c("Estimate of k" = k_hat_lin, "Estimate of psi" = psi_hat_lin))
```

We can visually check that the estimated regression coefficients are reasonably estimated by plotting the fitted regression line on top of the data:

```{r echo = FALSE, fig.width = 5, fig.height = 4, fig.align = 'center'}
ggplot(data = Rain.data.grp, aes(x = log(meanY), y = log(varY))) +
  geom_point() +
  geom_smooth(method = "lm", formula = y ~ x, se = FALSE) +
  xlab("log(mean(Y))") +
  ylab("log(var(Y))") +
  ggtitle("log(var(Y)) against log(mean(Y))") +
  theme_bw()
```

We see no warning signs from the plot and is fair to assume, that the value of $k$ and $\psi$ are descent estimates of the true values.

## Fitting a Tweedie model to the data

We proceed to fit a Tweedie model to the data. We use the `tweedie()` family specification from the `tweedie` package to fit the model with the estimated value of $k$ and `link.power = 0` for the log-link.

```{r}
tweedie.fit <- glm(Rain ~ Phase, data = Rain.data.comp, 
                   family = tweedie(var.power = k_hat_lin, link.power = 0))
pander(summary(tweedie.fit))
```

To interpret the model output we recall the five SOI levels

+ Phase 1: Consistently negative
+ Phase 2: Consistently positive
+ Phase 3: Rapidly falling
+ Phase 4: Rapidly rising
+ Phase 5: Consistently near zero

Note that Phase1 is taken to be the reference phase. The model suggests that the rainfall for Phase 1 is significantly different from 0 with a point estimate of the average rainfall of 

```{r echo = FALSE}
predict(tweedie.fit, type = "response", newdata = data.frame(Phase = as.factor(1)))
```

millimeters in July. The model estimates that rainfall for Phase 2 is significantly different from the rainfall in Phase 1 with a point estimate of average rainfall of 

```{r echo = FALSE}
predict(tweedie.fit, type = "response", newdata = data.frame(Phase = as.factor(2)))
```

millimeters in July when SOI is in this phase. The model estimates that the rainfall for Phase 3 is not significantly different from the rainfall in Phase 1 with a point estimate of rainfall of

```{r echo = FALSE}
predict(tweedie.fit, type = "response", newdata = data.frame(Phase = as.factor(3)))
```
millimeters on average in July. The rainfall for Phase 4 is significantly different from the rainfall in Phase 1 according to the model with a point estimate of average rainfall of 

```{r echo = FALSE}
predict(tweedie.fit, type = "response", newdata = data.frame(Phase = as.factor(4)))
```
millimeters in July. The model estimates rainfall for Phase 5 to be borderline significantly different from rainfall in Phase 1 with a point estimate of average rainfall of 

```{r echo = FALSE}
predict(tweedie.fit, type = "response", newdata = data.frame(Phase = as.factor(5)))
```

millimeters in July. Hence, the model suggests that Phase 2 and Phase 4 leads to significantly more rain on average than Phase 1. Phase 2 and 4 are when the SOI is consistently positive and rapidly rising respectively. A bit less rain seem to fall on average in Phase 5 when the SOI is consistently near zero, while the model predicts least rain on average in SOI phases 1 and 3 which are estimated not to be significantly different.

Note that the model predicts $\psi$ to be

```{r}
psi_hat_tweedie.fit <- summary(tweedie.fit)$dispersion
psi_hat_tweedie.fit
```

which is slightly different from the result obtained from the linear regression where $\psi$ was estimated to be

```{r}
psi_hat_lin
```


## Estimating probability of zero rain in July
We use the estimates obtained in the previous exercises to estimate the probability that it will not rain in July. In the theoretical exercises we derived the probability of zero rain to be

$$ \mathbb{P}(Y = 0) = \exp(-\lambda^*) = \exp(-\frac{\mu^{2-k}}{\psi (2-k)}) $$

With the two estimates of $\psi$ from the previous exercise, we compute two estimates of the the probability that it will not rain in July. We plug in the estimated values of $k$ and the empirical mean of our data:

```{r}
mu_hat <- mean(Rain.data.comp$Rain)

exp(-mu_hat^(2 - k_hat_lin)/(psi_hat_lin*(2 - k_hat_lin)))
exp(-mu_hat^(2 - k_hat_lin)/(psi_hat_tweedie.fit*(2 - k_hat_lin)))
```

The two results $34.1$ pct. and $35.5$ pct. are quite similar and compared to the empirical probability of zero rain in July

```{r}
sum(Rain.data.comp$Rain == 0)/nrow(Rain.data.comp)
```

we obtain three estimates that are all very similar. 


## Determining k by minimizing AIC
We now estimate $k$ by minimizing the Akaike Information Criterion (AIC) with a profile likelihood of a model with SOI phase as explanatory variable. That is, we search for the value of $k \in (1,2)$ that minimizes the AIC. We start by constructing a general profile likelihood function that takes inputs: A formula, a family, a data set and an evaluation metric that we wish to optimize.

```{r}
profile_likelihood <- function(form, family, data, eval) {
  model <- glm(form, 
               family = family, 
               data = data)
  eval_val <- eval(model)
  return(eval_val)
}
```

We define the specific profile likelihood that minimizes the AIC for different values of $k$ of a Tweedie exponential model with rainfall as response and SOI phase as covariate.  We specify `form = Rain ~ Phase`, `family = tweedie(var.power = k, link.power = 0)`, `data = Rain.data.comp` and `eval = AICtweedie`:

```{r}
tweedie.AIC_profile_likelihood <- function(k) {
  profile_likelihood(form = Rain ~ Phase, 
                         family = tweedie(var.power = k, link.power = 0), 
                         data = Rain.data.comp,
                         eval = AICtweedie)
}
```

We use the `optimize` function to minimize the AIC and find the optimal value of $k$, where we search for $k$ in the range $(1.05, 1.95)$.

```{r}
k_hat_AIC <- optimize(tweedie.AIC_profile_likelihood, lower = 1.05, upper = 1.95)$minimum
k_hat_AIC
```

We note that the optimal value of $k$ using the profile likelihood method is fairly close to the value of $k$ estimated by the linear model

```{r}
k_hat_lin
```

To ensure, that we have found a minimum for $1<k<2$ we plot the AIC against $k$ and add a vertical and a horizontal line at the optimal value of $k$.

```{r echo = FALSE, warning = FALSE, message = FALSE, fig.width = 6, fig.height = 5, fig.align = 'center'}
k_plot_df <- data.frame(k = seq(1.05, 1.95, by = 0.001), AIC = sapply(seq(1.05, 1.95, by = 0.001), FUN = tweedie.AIC_profile_likelihood))

k_hat_AIC_obj <- optimize(tweedie.AIC_profile_likelihood, lower = 1.05, upper = 1.95)$objective

ggplot(data = k_plot_df, aes(x = k, y = AIC)) +
  geom_line() +
  xlab("k") +
  ylab("AIC") +
  ggtitle("AIC against k") +
  theme_bw() +
  geom_vline(xintercept = k_hat_AIC, linetype = "dashed") +
  geom_hline(yintercept = k_hat_AIC_obj, linetype = "dashed") +
  geom_text(aes(x = k_hat_AIC + 0.01, y = 835, label = paste0("Optimal value of k at ", round(k_hat_AIC,3))),
            hjust = 0, vjust = 0) +
  geom_text(aes(x = 1.85, y = k_hat_AIC_obj + 20, label = paste0("Minimum AIC at ", round(k_hat_AIC_obj,3))))
```
The plot confirms, that `optimize()` has found the global minimum of the AIC for $1<k<2$. We repeat the calculations from the previous exercises using the optimal value of $k$ found by minimizing the profile likelihood.


## Re-estimating with new value of k
As before we initially fit a Tweedie model to the data using the optimal value of $k$ found by minimizing the AIC.

```{r}
tweedie.fit.AIC <- glm(Rain ~ Phase, data = Rain.data.comp, 
                       family = tweedie(var.power = k_hat_AIC, link.power = 0))
pander(summary(tweedie.fit.AIC))
```

We note a slight decrease in standard error and p-value for all coefficients and the estimated dispersion parameter $\hat \psi_{k_{AIC}} = 7.740$ is slightly different from the previous Tweedie model estimate $\hat \psi_{k_{lin}} = 8.212$. Apart from that the results are identical to the previous model and the interpretation is the same.

We recalculate the estimated probability that it will not rain in July. We plug in the estimated values of $k$ and $\psi$ and the empirical mean of our data we get the following estimate of the probability that it will not rain:

```{r}
psi_hat_model_AIC <- summary(tweedie.fit.AIC)$dispersion
exp(-mu_hat^(2 - k_hat_AIC)/(psi_hat_lin*(2 - k_hat_AIC)))
exp(-mu_hat^(2 - k_hat_AIC)/(psi_hat_model_AIC*(2 - k_hat_AIC)))
```

Again the estimates are similar to the estimates obtained from the previous estimate of $k$.


## Model diagnostics
We check the model assumptions for the two Tweedie models fitted in the previous exercise. First we construct a data frame with the relevant diagnostic information for both Tweedie models. We extract the fitted values, Pearson residuals and deviance residuals for both models and add the SOI phase as a variable to the data frame.

```{r echo = FALSE}
linmod_diag <- fortify(tweedie.fit)
AIC_diag <- fortify(tweedie.fit.AIC)

diag_df <- data.frame(linmod.fitted = linmod_diag$.fitted,
                      linmod.pearson = residuals(tweedie.fit, type = "pearson"),
                      linmod.deviance = residuals(tweedie.fit, type = "deviance"),
                      AIC.fitted = AIC_diag$.fitted,
                      AIC.pearson = residuals(tweedie.fit.AIC, type = "pearson"),
                      AIC.deviance = residuals(tweedie.fit.AIC, type = "deviance"),
                      SOI = Rain.data.comp$SOI)

pander(summary(diag_df))
```

We then plot the residuals against the fitted values first for the linear model estimate of $k$ and then for the AIC estimate of $k$.


### Linear model estimate of k

```{r, warning=FALSE, echo = FALSE, message = FALSE, fig.width = 10, fig.height = 6, fig.align = 'center'}
diag_lin_DR_fitted <- qplot(x = linmod.fitted, y = linmod.deviance, data = diag_df) +
  geom_point() +
  geom_smooth() +
  xlab("Fitted values") +
  ylab("Deviance residuals") +
  ggtitle("Deviance residuals against fitted values") +
  theme_bw() +
  theme(plot.title=element_text(size=10))

diag_lin_PR_fitted <- qplot(x = linmod.fitted, y = linmod.pearson, data = diag_df) +
  geom_point() +
  geom_smooth() +
  xlab("Fitted values") +
  ylab("Deviance residuals") +
  ggtitle("Pearson residuals against fitted values") +
  theme_bw() +
  theme(plot.title=element_text(size=10))

diag_lin_DR_SOI <- qplot(x = SOI, y = linmod.deviance, data = diag_df) +
  geom_point() +
  geom_smooth() +
  xlab("SOI") +
  ylab("Deviance residuals") +
  ggtitle("Deviance residuals against SOI") +
  theme_bw() +
  theme(plot.title=element_text(size=10))

diag_lin_PR_SOI <- qplot(x = SOI, y = linmod.pearson, data = diag_df) +
  geom_point() +
  geom_smooth(size = 1) +
  xlab("SOI") +
  ylab("Pearson residuals") +
  ggtitle("Pearson residuals against SOI") +
  theme_bw() +
  theme(plot.title=element_text(size=10))

grid.arrange(diag_lin_DR_fitted, diag_lin_PR_fitted, diag_lin_DR_SOI, diag_lin_PR_SOI, ncol = 2,
             top = textGrob(paste0("k = ", round(k_hat_lin, 3))))
```

From the plots there is no clear indication that the model is misspecified. The residuals appear to be randomly scattered around zero which indicates that the model captures the mean and variance structure of the data. We further evaluate the plots with bootstrapping. In particular we simulate data from the fitted model. That is, assuming data is from a Tweedie distribution with the estimated mean, dispersion parameter and $k$ we simulate new data and fit a Tweedie model to the new data. We then plot the residuals against the fitted values and the SOI phase for the new data. We hope to see, that the bootstrapped residuals are similar to the residuals from the original data.

```{r warning=FALSE, echo = FALSE, message = FALSE, fig.width = 10, fig.height = 6, fig.align = 'center'}
set.seed(10102024)

# Retrieve the estimated mean value of Y given the covariates
linmu_hats <- predict(tweedie.fit, type = "response")

# Use the estimated mean value of Y given the covariates, the estimate of k and the estimated dispersion parameter to simulate new data
yNew1 <- rTweedie(linmu_hats, psi_hat_tweedie.fit, p = k_hat_lin)
yNew2 <- rTweedie(linmu_hats, psi_hat_tweedie.fit, p = k_hat_lin)
yNew3 <- rTweedie(linmu_hats, psi_hat_tweedie.fit, p = k_hat_lin)
yNew4 <- rTweedie(linmu_hats, psi_hat_tweedie.fit, p = k_hat_lin)
yNew5 <- rTweedie(linmu_hats, psi_hat_tweedie.fit, p = k_hat_lin)
yNew6 <- rTweedie(linmu_hats, psi_hat_tweedie.fit, p = k_hat_lin)
simGlmNew1 <- glm(yNew1 ~ Rain.data.comp$Phase, tweedie(var.power = k_hat_lin, link.power = 0))
simGlmNew2 <- glm(yNew2 ~ Rain.data.comp$Phase, tweedie(var.power = k_hat_lin, link.power = 0))
simGlmNew3 <- glm(yNew3 ~ Rain.data.comp$Phase, tweedie(var.power = k_hat_lin, link.power = 0))
simGlmNew4 <- glm(yNew4 ~ Rain.data.comp$Phase, tweedie(var.power = k_hat_lin, link.power = 0))
simGlmNew5 <- glm(yNew5 ~ Rain.data.comp$Phase, tweedie(var.power = k_hat_lin, link.power = 0))
simGlmNew6 <- glm(yNew6 ~ Rain.data.comp$Phase, tweedie(var.power = k_hat_lin, link.power = 0))
simDiagNew1 <- fortify(simGlmNew1)
simDiagNew2 <- fortify(simGlmNew2)
simDiagNew3 <- fortify(simGlmNew3)
simDiagNew4 <- fortify(simGlmNew4)
simDiagNew5 <- fortify(simGlmNew5)
simDiagNew6 <- fortify(simGlmNew6)
p1 <- qplot(.fitted, .resid, data = simDiagNew1) + xlab("Fitted values") +
  ylab("Deviance residuals") + geom_smooth() + theme_bw()
p2 <- qplot(.fitted, .resid, data = simDiagNew2) + xlab("Fitted values") +
  ylab("Deviance residuals") + geom_smooth() + theme_bw()
p3 <- qplot(.fitted, .resid, data = simDiagNew3) + xlab("Fitted values") +
  ylab("Deviance residuals") + geom_smooth() + theme_bw()
p4 <- qplot(.fitted, .resid, data = simDiagNew4) + xlab("Fitted values") +
  ylab("Deviance residuals") + geom_smooth() + theme_bw()
p5 <- qplot(.fitted, .resid, data = simDiagNew5) + xlab("Fitted values") +
  ylab("Deviance residuals") + geom_smooth() + theme_bw()
p6 <- qplot(.fitted, .resid, data = simDiagNew6) + xlab("Fitted values") +
  ylab("Deviance residuals") + geom_smooth() + theme_bw()

grid.arrange(p1, p2, p3, p4, p5, p6, ncol = 3,
             top = textGrob("Bootstrap deviance residual plots"))
```
We see that the plots by and large resemble the residual plots of the original model. This supports the initial diagnostic plots. That is, there is no clear evidence that the model assumptions are violated.

### AIC estimate of k

```{r, warning=FALSE, echo = FALSE, message = FALSE, fig.width = 10, fig.height = 6, fig.align = 'center'}
diag_AIC_DR_fitted <- qplot(x = AIC.fitted, y = AIC.deviance, data = diag_df) +
  geom_point() +
  geom_smooth() +
  xlab("Fitted values") +
  ylab("Deviance residuals") +
  ggtitle("Deviance residuals against fitted values") +
  theme_bw() +
  theme(plot.title=element_text(size=10))


diag_AIC_PR_fitted <- qplot(x = AIC.fitted, y = AIC.pearson, data = diag_df) +
  geom_point() +
  geom_smooth() +
  xlab("Fitted values") +
  ylab("Deviance residuals") +
  ggtitle("Pearson residuals against fitted values") +
  theme_bw() +
  theme(plot.title=element_text(size=10))

diag_AIC_DR_SOI <- qplot(x = SOI, y = AIC.deviance, data = diag_df) +
  geom_point() +
  geom_smooth() +
  xlab("SOI") +
  ylab("Deviance residuals") +
  ggtitle("Deviance residuals against SOI") +
  theme_bw() +
  theme(plot.title=element_text(size=10))

diag_AIC_PR_SOI <- qplot(x = SOI, y = AIC.deviance, data = diag_df) +
  geom_point() +
  geom_smooth(size = 1) +
  xlab("SOI") +
  ylab("Deviance residuals") +
  ggtitle("Deviance residuals against SOI") +
  theme_bw() +
  theme(plot.title=element_text(size=10))

grid.arrange(diag_AIC_DR_fitted, diag_AIC_PR_fitted, diag_AIC_DR_SOI, diag_AIC_PR_SOI, ncol = 2,
             top = textGrob(paste0("k = ", round(k_hat_AIC, 3))))
```

We notice a pattern similar to the previous model.

```{r warning=FALSE, echo = FALSE, message = FALSE, fig.width = 10, fig.height = 6, fig.align = 'center'}
set.seed(10102024)

AICmu_hats <- predict(tweedie.fit.AIC, type = "response")

AICyNew1 <- rTweedie(AICmu_hats, psi_hat_model_AIC, p = k_hat_AIC)
AICyNew2 <- rTweedie(AICmu_hats, psi_hat_model_AIC, p = k_hat_AIC)
AICyNew3 <- rTweedie(AICmu_hats, psi_hat_model_AIC, p = k_hat_AIC)
AICyNew4 <- rTweedie(AICmu_hats, psi_hat_model_AIC, p = k_hat_AIC)
AICyNew5 <- rTweedie(AICmu_hats, psi_hat_model_AIC, p = k_hat_AIC)
AICyNew6 <- rTweedie(AICmu_hats, psi_hat_model_AIC, p = k_hat_AIC)
AICsimGlmNew1 <- glm(AICyNew1 ~ Rain.data.comp$Phase, tweedie(var.power = k_hat_AIC, link.power = 0))
AICsimGlmNew2 <- glm(AICyNew2 ~ Rain.data.comp$Phase, tweedie(var.power = k_hat_AIC, link.power = 0))
AICsimGlmNew3 <- glm(AICyNew3 ~ Rain.data.comp$Phase, tweedie(var.power = k_hat_AIC, link.power = 0))
AICsimGlmNew4 <- glm(AICyNew4 ~ Rain.data.comp$Phase, tweedie(var.power = k_hat_AIC, link.power = 0))
AICsimGlmNew5 <- glm(AICyNew5 ~ Rain.data.comp$Phase, tweedie(var.power = k_hat_AIC, link.power = 0))
AICsimGlmNew6 <- glm(AICyNew6 ~ Rain.data.comp$Phase, tweedie(var.power = k_hat_AIC, link.power = 0))
AICsimDiagNew1 <- fortify(AICsimGlmNew1)
AICsimDiagNew2 <- fortify(AICsimGlmNew2)
AICsimDiagNew3 <- fortify(AICsimGlmNew3)
AICsimDiagNew4 <- fortify(AICsimGlmNew4)
AICsimDiagNew5 <- fortify(AICsimGlmNew5)
AICsimDiagNew6 <- fortify(AICsimGlmNew6)
AICp1 <- qplot(.fitted, .resid, data = AICsimDiagNew1) + xlab("Fitted values") +
  ylab("Deviance residuals") + geom_smooth() + theme_bw()
AICp2 <- qplot(.fitted, .resid, data = AICsimDiagNew2) + xlab("Fitted values") +
  ylab("Deviance residuals") + geom_smooth() + theme_bw()
AICp3 <- qplot(.fitted, .resid, data = AICsimDiagNew3) + xlab("Fitted values") +
  ylab("Deviance residuals") + geom_smooth() + theme_bw()
AICp4 <- qplot(.fitted, .resid, data = AICsimDiagNew4) + xlab("Fitted values") +
  ylab("Deviance residuals") + geom_smooth() + theme_bw()
AICp5 <- qplot(.fitted, .resid, data = AICsimDiagNew5) + xlab("Fitted values") +
  ylab("Deviance residuals") + geom_smooth() + theme_bw()
AICp6 <- qplot(.fitted, .resid, data = AICsimDiagNew6) + xlab("Fitted values") +
  ylab("Deviance residuals") + geom_smooth() + theme_bw()

grid.arrange(AICp1, AICp2, AICp3, AICp4, AICp5, AICp6, ncol = 3,
             top = textGrob("Bootstrap deviance residual plots"))
```

Again the pattern appears to be similar to the previous model. There may be concerns in the second and fifth plot where the model residuals from the simulated data diverge slightly from the original model. It is difficult to say if this is just noise or if the second estimate of $k$ is worse that the first estimated $k$.

## Conclusion
The above analysis suggest there is an association between rainfall and SOI phase. This hypothesis was visible from the boxplot of rainfall and SOI phase in the EDA and was further supported by the models in the subsequent analysis. Given the model diagnostics it appears that both fitted models capture the mean and variance structure of the data and we it is therefore can therefore trust the model conclusions. We have not tested the distributional assumptions of the model and the reported standard error of the model estimates are therefore therefore not to be trusted yet.


## Bootstrap estimates of k

We conclude this part of the analysis with parametric bootstrap to estimate the sampling distribution of $k$. Using parameters estimated by a Tweedie model, we simulate data from a Tweedie distribution and fit a new Tweedie model to the simulated data to estimate $k$. We repeat this process multiple times to estimate the sampling distribution of $k$. To carry out the parametric bootstrap we need the strong distributional assumptions GA3 and A5. We have not formally tested these assumptions, so further analysis should check their validity. For now we acknowledge, that the parametric bootstrap may produce too narrow confidence intervals.

We attempt to estimate the sampling distribution of $k$ using the AIC profile likelihood method. Due to the small number of observations in phase 3 there is a high probability, that some bootstrap samples will contain only zeros for phase 3. This implies, that the fitted model used in the profile-likelihood will fail to converge for these bootstrap samples. To mitigate this issue, we add a small random value to the zeros in a group, in case they are all zeros. While this method introduces some bias, we prefer this method over relying on estimates from a non-converged algorithm which would provide meaningless estimates.

```{r}
set.seed(10102024)
parametric_k_AIC <- numeric(B)
rain_data_AIC_sample <- Rain.data.comp


for (b in 1:B){
  rain_data_AIC_sample$Rain <- rTweedie(AICmu_hats, 
                                        psi_hat_model_AIC, 
                                        p = k_hat_AIC)
  
  
  # Calculate sum for each phase in order to handle zeros
  rain_data_AIC_sample <- rain_data_AIC_sample %>%
    group_by(Phase) %>%
    mutate(phase_sum = sum(Rain)) %>%
    ungroup()

  # Add small constant to phases where sum of Rain is zero to ensure convergence 
  rain_data_AIC_sample <- rain_data_AIC_sample %>%
    mutate(Rain = if_else(phase_sum == 0,
                          Rain + abs(rnorm(n(), 0.0001, 0.001)),
                          Rain))

  tweedieBootstrap.AIC_profile_likelihood <- function(k) {
    profile_likelihood(form = Rain ~ Phase, 
                       family = tweedie(var.power = k, link.power = 0), 
                       data = rain_data_AIC_sample,
                       eval = AICtweedie)
  }
  
  boot_k_hat <- optimize(tweedieBootstrap.AIC_profile_likelihood, 
                         lower = 1.1, upper = 1.9)$minimum
  
  parametric_k_AIC[b] <- boot_k_hat
}
```



```{r warning=FALSE, echo = FALSE, message = FALSE, fig.width = 10, fig.height = 6, fig.align = 'center'}
violin_df <- data.frame(k = c(parametric_k_AIC),
                        method = as.factor(rep(c("AIC"), each = B)))

ggplot(violin_df, aes(x = method, y = k)) +
  geom_violin(fill = I(gray(0.8))) + xlab("") +
  facet_wrap(~ method, scale = "free_x", ncol = 2) + theme_bw() +
  geom_hline(yintercept = k_hat_AIC, color = "red", linetype = "dashed") +
  annotate("text", x = 1, y = k_hat_AIC, 
           label = "AIC estimate of k", color = "red", vjust = -0.5)
```

It appears that the AIC method is fairly robust to changes in the bootstrapped samples with the exception of the spike in the plot that comes from the samples where phase 3 has only zero observations. We use the parametric AIC bootstrap to estimate the standard error of the AIC estimates of $k$:

```{r}
se_AIC <- sd(parametric_k_AIC)
se_AIC
```

```{r}
k_hat_AIC + c(-1, 1) * 1.96 * se_AIC
```

To conclude, it appears the the AIC profile likelihood method is a more robust method of estimating $k$ that the linear model approach. The AIC estimate of $k$ is $1.42$ with standard error $0.07$ and a parametric bootstrap confidence interval of $(1.28, 1.57)$.

# Analysis using SOI directly

We choose to model the rainfall conditionally on having rained. To do this we first filter out the observations where the rainfall is zero. 

```{r warning = FALSE, echo = TRUE, message = FALSE}
rain.data1 <- Rain.data.comp %>%
  filter(Rain != 0)
```

We look at the distribution of the rain fall. 

```{r warning = FALSE, echo = FALSE, message = FALSE, fig.width = 10, fig.height = 6, fig.align = 'center'}
ggplot(data = rain.data1, aes(x = Rain)) +
  geom_density(fill = gray(0.5)) +
  xlab("Rain") +
  ylab("Density") +
  ggtitle("Rain fall distribution")
```
The distribution is still very right skewed. This motivates the Gamma exponential dispersion model, which is typically used to fit positive continous right skewed data. Consider now the rain fall as a function of the SOI. 

```{r warning = FALSE, echo = FALSE, message = FALSE, fig.width = 10, fig.height = 6, fig.align = 'center'}
# Plot rain against year using ggplot
ggplot(data = rain.data1, aes(x = SOI, y = Rain)) +
  geom_point() +
  geom_smooth() +
  xlab("SOI") +
  ylab("Rain") + 
  ggtitle("Rain against SOI")

```
There seems to be a positive relationship between the SOI and the rain fall. The plot also indicates a possibility of non linear trends. The Gamma exponential dispersion model has a quadratic mean variance relationsship, which also seems like a good fit from the plot above. 

If we choose the log link function, we fit the log of the mean of the response variables as a linear combination of the predictors. Other options include the identity link and the canonical link, which is the inverse function. A problem we have encountered when using the identity link is that for some parameters the model produces negative predictions, which is not possible for the rain fall, and furthermore causes convergence issues. For this reason the identity link is disregarded. We fit two models: one with log link and one using the canonical link. 

```{r}
glm_gamma_log <- glm(Rain ~ SOI, data = rain.data1, family = Gamma("log"))
glm_gamma_inv <- glm(Rain ~ SOI, data = rain.data1, family = Gamma)
```

Below we calculate the training error based on the squared deviance loss function. 

```{r warning = FALSE, echo = FALSE, message = FALSE, fig.width = 10, fig.height = 6, fig.align = 'center'}
tibble(
  "Link function" = c("Log", "Inverse"),
  "Training error" = c(mean(residuals(glm_gamma_log, type = "deviance")^2), 
                       mean(residuals(glm_gamma_inv, type = "deviance")^2))
) %>% pander()
```
The log link function seems to be the best fit in terms of training error. We plot the model fits:

```{r warning = FALSE, echo = FALSE, message = FALSE, fig.width = 10, fig.height = 6, fig.align = 'center'}
log_pred <- predict(glm_gamma_log, newdata = rain.data1, type = "response")
inv_pred <- predict(glm_gamma_inv, newdata = rain.data1, type = "response")

grid.arrange(
  qplot(rain.data1$SOI, rain.data1$Rain) + 
    geom_line(aes(y = log_pred), color = "red") +
    xlab("SOI") +
    ylab("Rain")+
    ggtitle("Log link function"),
   qplot(rain.data1$SOI, rain.data1$Rain) + 
    geom_line(aes(y = inv_pred), color = "red") +
    xlab("SOI") +
    ylab("Rain") +
    ggtitle("Inverse link function"),
  nrow = 1
)
```
The Gamma model with log link function seems to fit data reasonably well. We choose to proceed with the Gamma model rather than the log link function. 

## Should we include additional predictors?

We perform an LRT test to see if we should include additional predictors. We consider to add the predictors Phase and Year. As mentioned in the EDA, Phase and SOI are very correlated, and it could be problematic to include both in the model.  

```{r}
add1(glm_gamma_log, Rain ~ SOI + Phase + Year, test = "LRT") %>% kable()
```
According to the LRT test there is not evidence in data that suggest that the additional predictors should be added to the model. 

## Explore possible inclusion of nonlinear effects

We consider to include a non-linear effect of SOI. We consider a natural cubic spline with 2,3,4,5 or 6 degrees of freedom:

```{r warning = FALSE, echo = TRUE, message = FALSE, fig.width = 10, fig.height = 6, fig.align = 'center'}
form1 <- Rain ~ SOI
form2 <- Rain ~ ns(SOI, df = 2)
form3 <- Rain ~ ns(SOI, df = 3)
form4 <- Rain ~ ns(SOI, df = 4)
form5 <- Rain ~ ns(SOI, df = 5)
form6 <- Rain ~ ns(SOI, df = 6)

glm1 <- glm(form1, data = rain.data1, family = Gamma("log"))
glm2 <- glm(form2, data = rain.data1, family = Gamma("log"))
glm3 <- glm(form3, data = rain.data1, family = Gamma("log"))
glm4 <- glm(form4, data = rain.data1, family = Gamma("log"))
glm5 <- glm(form5, data = rain.data1, family = Gamma("log"))
glm6 <- glm(form6, data = rain.data1, family = Gamma("log"))
```

The model fits: 

```{r warning = FALSE, echo = FALSE, message = FALSE, fig.width = 10, fig.height = 6, fig.align = 'center'}
pred1 <- predict(glm1, newdata = rain.data1, type = "response")
pred2 <- predict(glm2, newdata = rain.data1, type = "response")
pred3 <- predict(glm3, newdata = rain.data1, type = "response")
pred4 <- predict(glm4, newdata = rain.data1, type = "response")
pred5 <- predict(glm5, newdata = rain.data1, type = "response")
pred6 <- predict(glm6, newdata = rain.data1, type = "response")

grid.arrange(
  qplot(rain.data1$SOI, rain.data1$Rain) + 
    geom_line(aes(y = pred1), color = "red") +
    xlab("SOI") +
    ylab("Rain")+
    ggtitle("No expansion"),
   qplot(rain.data1$SOI, rain.data1$Rain) + 
    geom_line(aes(y = pred2), color = "red") +
    xlab("SOI") +
    ylab("Rain") +
    ggtitle("Df = 2"),
   qplot(rain.data1$SOI, rain.data1$Rain) + 
    geom_line(aes(y = pred3), color = "red") +
    xlab("SOI") +
    ylab("Rain") +
    ggtitle("Df = 3"),
  qplot(rain.data1$SOI, rain.data1$Rain) + 
    geom_line(aes(y = pred4), color = "red") +
    xlab("SOI") +
    ylab("Rain") +
    ggtitle("Df = 4"),   
  qplot(rain.data1$SOI, rain.data1$Rain) + 
    geom_line(aes(y = pred5), color = "red") +
    xlab("SOI") +
    ylab("Rain") +
    ggtitle("Df = 5"),
  qplot(rain.data1$SOI, rain.data1$Rain) + 
    geom_line(aes(y = pred6), color = "red") +
    xlab("SOI") +
    ylab("Rain") +
    ggtitle("Df = 6"),
  nrow = 2
)
```

Adding more degrees of freedom to the natural cubic splines adds flexibility to the model, allowing it to fit data better. This comes at the expense of potentially overfitting. The model fitted with 6 degress of freedom is quite likely overfitting data. But in order to better assess which model is the best in terms of prediction, we do cross-validation to compare the models. We first define the error function. We use the deviance loss function.

```{r}
# Error function
dev_loss <- function(Y, muhat) 2 * (log(muhat / Y) + Y / muhat - 1)
```

We define the cross validation function. 

```{r}
cv <- function(data, form, B = 1, k = 8, my_family, error_func){
  n <- nrow (data)
  PEcv <- vector("list", B)
  tmp <- numeric(n)
  for (b in 1: B){
    ## Generating the random division into groups
    group <- sample(rep(1:k, length.out = n))
    for (i in 1:k){
      modelcv <- glm(form, data = data[group != i, ], family = my_family)
      muhat <- predict(modelcv, newdata = data[group == i, ], type = "response")
      # !!! change input of error function !!!
      tmp[group == i] <- error_func(data$Rain[group == i],  muhat)
    }
    PEcv[[b]] <- tmp
  }
  mean(unlist(PEcv))
}
```

Since the data set is quite small, we perform LOOCV. This is a non random procedure and we therefore set $B = 1$.

```{r warning = FALSE, echo = TRUE, message = FALSE, fig.width = 10, fig.height = 6, fig.align = 'center'}
cv(data = rain.data1, form = form1, B = 1, k = nrow(rain.data1), 
   my_family = Gamma("log"), 
   error_func = dev_loss)

cv(data = rain.data1, form = form2, B = 1, k = nrow(rain.data1), 
   my_family = Gamma("log"), 
   error_func = dev_loss)

cv(data = rain.data1, form = form3, B = 1, k = nrow(rain.data1), 
   my_family = Gamma("log"), 
   error_func = dev_loss)

cv(data = rain.data1, form = form4, B = 1, k = nrow(rain.data1), 
   my_family = Gamma("log"), 
   error_func = dev_loss)

cv(data = rain.data1, form = form5, B = 1, k = nrow(rain.data1), 
   my_family = Gamma("log"), 
   error_func = dev_loss)

cv(data = rain.data1, form = form6, B = 1, k = nrow(rain.data1), 
   my_family = Gamma("log"), 
   error_func = dev_loss)
```
We proceed with the model with 2 degrees of freedom as it has the smallest cross validation error. But note that the model without natural cubic splines is very close to performing just as well in terms of generalization error. Since we are interested in prediction, we choose the model with 2 degrees of freedom. If interpretability was of higher priority we would choose the model without natural cubic splines. 

## Model Diagnostics

We do model diagnostics for the chosen model. 

```{r warning = FALSE, echo = TRUE, message = FALSE, fig.width = 10, fig.height = 6, fig.align = 'center'}
mod_final <- glm(form2, data = rain.data1, family = Gamma("log"))

glm_gamma.obj <- transform(
  rain.data1,
  .fitted = predict(mod_final, type = "response"),
  .deviance = residuals(mod_final, type = "deviance"),
  .pearson = residuals(mod_final, type = "pearson")
)

grid.arrange(
  qplot(.fitted, .deviance, data = glm_gamma.obj) + geom_smooth(),
  qplot(.fitted, .pearson, data = glm_gamma.obj) + geom_smooth(),
  qplot(.fitted, sqrt(abs(.pearson)), data = glm_gamma.obj) + geom_smooth(),
  ncol=3
)
```

The model diagnostics look quite good. There is no clear evidence against the model assumptions. We evaluate these plots via bootstrapping. We compare the residuals with simulated residuals under the null hypothesis that our model is correct.

```{r warning = TRUE, echo = FALSE, message = FALSE, fig.width = 10, fig.height = 6, fig.align = 'center'}
set.seed(1986234)
glm_list <- list()
for(i in 1:6){
  sim_data <- simulate(mod_final)[,1]
  glm_list[[i]] <- glm(sim_data ~ ns(rain.data1$SOI, df = 2),
                       family = Gamma(link = "log"))
}

grid.arrange(
  qplot(fitted(glm_list[[1]]), residuals(glm_list[[1]], type = "deviance")) + 
    xlab("Fitted values") +
    ylab("Deviance residuals") +
    geom_smooth(),
  qplot(fitted(glm_list[[2]]), residuals(glm_list[[2]], type = "deviance")) + 
    xlab("Fitted values") +
    ylab("Deviance residuals") +
    geom_smooth(),
  qplot(fitted(glm_list[[3]]), residuals(glm_list[[3]], type = "deviance")) + 
    xlab("Fitted values") +
    ylab("Deviance residuals") +
    geom_smooth(),
  qplot(fitted(glm_list[[4]]), residuals(glm_list[[4]], type = "deviance")) + 
    xlab("Fitted values") +
    ylab("Deviance residuals") +
    geom_smooth(),
   qplot(fitted(glm_list[[5]]), residuals(glm_list[[5]], type = "deviance")) + 
    xlab("Fitted values") +
    ylab("Deviance residuals") +
    geom_smooth(),
  qplot(fitted(glm_list[[6]]), residuals(glm_list[[6]], type = "deviance")) + 
    xlab("Fitted values") +
    ylab("Deviance residuals") +
    geom_smooth(),
  ncol = 3
)

grid.arrange(
  qplot(fitted(glm_list[[1]]), residuals(glm_list[[1]], type = "pearson")) + 
    xlab("Fitted values") +
    ylab("Pearson residuals") +
    geom_smooth(),
  qplot(fitted(glm_list[[2]]), residuals(glm_list[[2]], type = "pearson")) + 
    xlab("Fitted values") +
    ylab("Pearson residuals") +
    geom_smooth(),
  qplot(fitted(glm_list[[3]]), residuals(glm_list[[3]], type = "pearson")) + 
    xlab("Fitted values") +
    ylab("Pearson residuals") +
    geom_smooth(),
  qplot(fitted(glm_list[[4]]), residuals(glm_list[[4]], type = "pearson")) + 
    xlab("Fitted values") +
    ylab("Pearson residuals") +
    geom_smooth(),
   qplot(fitted(glm_list[[5]]), residuals(glm_list[[5]], type = "pearson")) + 
    xlab("Fitted values") +
    ylab("Pearson residuals") +
    geom_smooth(),
  qplot(fitted(glm_list[[6]]), residuals(glm_list[[6]], type = "pearson")) + 
    xlab("Fitted values") +
    ylab("Pearson residuals") +
    geom_smooth(),
  ncol = 3
)

```
The bootstrapped residual plots resemble the original residual plots sufficiently well. This indicates that the fitted model is correct.

## Reporting a final model and interpretation

The final model fit

```{r warning = FALSE, echo = FALSE, message = FALSE, fig.width = 10, fig.height = 6, fig.align = 'center'}
summary(mod_final)
```

We see that SOI is a significant predictor of rain fall. Since we have used a natural cubic spline with 2 degrees of freedom to fit our model, the coefficients are difficult to interpret. We instead consider the predictions of the model. Below the model predictions for a few values of SOI are printed

```{r warning = FALSE, echo = FALSE, message = FALSE, fig.width = 10, fig.height = 6, fig.align = 'center'}
tibble(
  SOI = c(-20,-10,0,10,20),
  "Rain Fall" = predict(mod_final, 
                      newdata = data.frame(SOI = c(-20,-10,0,10,20)), 
                      type = "response")
) %>% pander()
```
A plot of the model fit is shown below.

```{r warning = FALSE, echo = FALSE, message = FALSE, fig.width = 10, fig.height = 6, fig.align = 'center'}
pred2 <- predict(mod_final, newdata = rain.data1, type = "response")

qplot(rain.data1$SOI, rain.data1$Rain) + 
  geom_line(aes(y = pred2), color = "red") +
  xlab("SOI") +
  ylab("Rain") +
  ggtitle("Rain ~ ns(SOI, df = 2)")
```

The fitted model predicts that rainfall is increasing as a function of SOI. The slope of the model is largest for values of SOI between $-10$ and $10$. For SOI values that are larger or smaller than this, the model is more constant. 

We will now turn to the construction of confidence intervals for our model. First we will use nonparametric bootstrap to create a combinant based confidence interval for the model predictions as described on page 220. 

```{r warning = FALSE, echo = TRUE, message = FALSE, fig.width = 10, fig.height = 6, fig.align = 'center'}
# Pair sampling
B <- 1000
set.seed(170)
n <- nrow(rain.data1)
boot_pred <- matrix(nrow = n, ncol = B)

for(b in 1:B){
  boot_samp <- sample(n, replace = TRUE)
  boot_mod <- glm(Rain ~ ns(SOI, df = 2), 
                  data = rain.data1[boot_samp, ], 
                  family = Gamma("log"))
  boot_pred[,b] <- predict(boot_mod, newdata = rain.data1, type = "response")
}

CIs <- matrix(nrow = n, ncol = 2)
for(i in 1:n){
  CIs[i,] <- 2*pred2[i] - quantile(boot_pred[i,], probs = c(0.975, 0.025), na.rm = TRUE)
}

p1 <- qplot(rain.data1$SOI, rain.data1$Rain) + 
  geom_line(aes(y = pred2), color = "black") +
  geom_ribbon(aes(ymin = CIs[,2], ymax = CIs[,1]), alpha = 0.3) +
  xlab("SOI") +
  ylab("Rain") +
  ggtitle("Pair sampling comb. based CI")
```

We will further construct confidence intervals of the form

$$
f \pm 1.96 \hat{\text{se}} 
$$
Note that these intervals will be symmetric around the point estimate. First we will use residual sampling to estimate standard errors of the model predictions and use these to construct the confidence interval. 
```{r warning = FALSE, echo = TRUE, message = FALSE, fig.width = 10, fig.height = 6, fig.align = 'center'} 
# Residual sampling
set.seed(170)
mus <- mod_final$fitted.values
res <- mod_final$residuals
boot_pred2 <- matrix(nrow = n, ncol = B)

for(b in 1:B){
  boot_samp <- sample(n, replace = TRUE)
  boot_data <- data.frame(Rain = mus + res[boot_samp], SOI = rain.data1$SOI)
  boot_mod <- glm(Rain ~ ns(SOI, df = 2), 
                  data = boot_data, 
                  family = Gamma("log"))
  boot_pred2[,b] <- predict(boot_mod, newdata = rain.data1, type = "response")
}

SEs <- numeric(n)
for(i in 1:n){
  SEs[i] <- sd(boot_pred2[i,])
}

CIs2 <- cbind(pred2 - SEs*qnorm(0.975), pred2 + SEs*qnorm(0.975))

p2 <- qplot(rain.data1$SOI, rain.data1$Rain) + 
  geom_line(aes(y = pred2), color = "black") +
  geom_ribbon(aes(ymin = CIs2[,2], ymax = CIs2[,1]), alpha = 0.3) +
  xlab("SOI") +
  ylab("Rain") +
  ggtitle("Res. samp. SE CI")
```

A last confidence interval we will consider is created in the same way as above, but using analytical standard errors of the model predictions. 

```{r warning = FALSE, echo = TRUE, message = FALSE, fig.width = 10, fig.height = 6, fig.align = 'center'}
SE_an <- predict(mod_final, newdata = rain.data1, type = "response", se.fit = TRUE)$se.fit
CIs3 <- cbind(pred2 - SE_an*qnorm(0.975), pred2 + SE_an*qnorm(0.975))

p3 <- qplot(rain.data1$SOI, rain.data1$Rain) + 
  geom_line(aes(y = pred2), color = "black") +
  geom_ribbon(aes(ymin = CIs3[,2], ymax = CIs3[,1]), alpha = 0.3) +
  xlab("SOI") +
  ylab("Rain") +
  ggtitle("Analytical SE CI")
```

We will now compare the three confidence intervals. 
```{r warning = FALSE, echo = FALSE, message = FALSE, fig.width = 10, fig.height = 6, fig.align = 'center'}
grid.arrange(p1, p2, p3, nrow = 1)
```

The first thing that catches the eye is that the confidence interval based on residual sampling SE estimates is very very narrow, which seems very unlikely. Apart from that we see that the two other confidence interval are narrow for the SOI values where we have many observations and wide for the SOI values for which we have few observations. This is to be expected. A last thing to point out is the asymmetry that is present in the bootstrap combinant based confidence interval, indicating a certain asymmetry of the distribution of the model predictions.

## Conclusion

Because of the right skew of data we decided to use the Gamma model to fit the data. We chose the log link as it ensures that the model predictions are kept within the domain of the distribution and since it performed well in terms of training error. We chose not to include further predictors as they were insignificant according to the LRT test.

The model we have fitted is a Gamma model with a log link function and a natural cubic spline with 2 degrees of freedom. This was the model with the smallest generalization error chosen by cross validation, where we considered models fitted on natural cubic splines with degrees of freedom ranging from 1 to 6.

The model is well fitted to the data and the residuals show no evidence against the model assumptions. 

The model predicts that rain fall is increasing as a function of SOI, and that SOI is a significant predictor of rain fall. We have constructed confidence intervals for the model predictions, which show that the model is most certain for SOI values where we have many observations, and less certain for SOI values where we have few observations.


