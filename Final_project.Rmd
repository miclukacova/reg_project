---
title: "Project in regression - practical part"
output:
  pdf_document:
    toc: yes
  html_document:
    code_folding: hide
    toc: yes
---

```{r warning = FALSE, message = FALSE, echo = FALSE}
#Loading relevant libraries
knitr::opts_chunk$set(results = "hold")

library(GLMsData)     # Project specific package
library(statmod)      # Project specific package
library(tweedie)      # Project specific package
library(mgcv)         # Project specific package

library(dplyr)        # For data manipulation
library(reshape2)     # For data manipulation
library(tidyverse)    # For data manipulation

library(splines)      # For splines

library(ggplot2)      # For plotting
theme_set(theme_bw()) # Set theme for ggplot2
library(gridExtra)    # For arranging plots
library(grid)         # For arranging plots
library(lattice)      # For plotting splom()
library(hexbin)       # For plotting hexbin

library(Hmisc)        # For multiple imputation
library(mice)         # For multiple imputation
library(sjmisc)       # For combining multiple imputation

library(pander)       # For printing tables
library(knitr)        # For printing tables
```


In this project we seek to predict the rainfall at Eromanga in Queensland, Australia, during the month of July. We consider a data set that contains total rainfall in July at Eromanga in the period 1905 to 2024 with the exception of a few years. In addition we have measurements of the southern oscillation index (SOI, the standardized difference between the air pressures at Darwin and Haiti, related to el ni√±o) for the same years. The hypothesis is that the SOI is related to the rainfall in Eromanga. We will throughout the project investigate this hypothesis and ultimately use the SOI index to predict the rainfall in Eromanga.

To increase readability we have omitted code chunks on several occasions. In particular when it comes to plots and other standard code parts. When the code and output is more involved and essential for the analysis and understanding of the project we have included it in the project. If anything is unclear all code can be accessed at https://github.com/miclukacova/reg_project.

# Exploratory data analysis

First, we load the data set

```{r}
Rain.data = read.table("RaindataEromanga.txt", 
                       header = TRUE, 
                       colClasses = c("integer", "numeric", "integer", "numeric", "factor"))
```

We specify the column classes and print the `head()` and `summary()` of the data set to make sure that the data is read in correctly.

```{r, echo = FALSE}
pander(head(Rain.data))
pander(summary(Rain.data))
```

The data set contains five variables: `Year`, `Rain`, `SOI`, `Phase` and `Month.` The `Month` variable is constant and will not be used or analyzed, but keep in mind, that the analysis is carried out for the month of July. The response variable `Rain` represents the total rainfall in July at Eromanga and contains nine missing values, which will be addressed in a subsequent part of the EDA. The `SOI` variable is the southern oscillation index, while `Phase` is a categorical variable indicating the SOI phase on five different levels:

+ Phase 1: Consistently negative
+ Phase 2: Consistently positive
+ Phase 3: Rapidly falling
+ Phase 4: Rapidly rising
+ Phase 5: Consistently near zero

We plot the marginal distributions of the variables to visually explore the data. 

```{r warning = FALSE, echo = FALSE, fig.width = 5.5, fig.height = 3, fig.align = 'center'}
tmp <- lapply(names(Rain.data), function(x)
  ggplot(data = Rain.data[, x, drop = FALSE]) +
    aes_string(x) +
    xlab(x) + 
    ylab("") +
    theme_bw()
  )

gd <- geom_density(adjust = 2, fill = gray(0.5))
gb <- geom_bar(fill = (gray(0.5)))

grid.arrange(
  tmp[[2]] + gd,
  tmp[[4]] + gd,
  tmp[[5]] + gb,
  ncol = 3
)
```

We observe that the response variable `Rain` is right skewed, which should be considered in later analysis. Additionally, Category 3 in `Phase` has relatively few observations, which may also require consideration in subsequent analysis. We proceed to investigate possible co-linearity between the variables in the data set. We assess correlation between the numerical variables `Rainfall`, `SOI` and `Year` with a correlation plot:

```{r warning = FALSE, echo = FALSE, fig.width = 3, fig.height = 3, fig.align = 'center'}
cor.print <- function(x, y){
  panel.text(mean(range(x)), mean(range(y)),
             paste(round(cor(x,y), digits = 2), sep = '')
             )
}
contVar <- c("Rain", "Year", "SOI")
splom(na.omit(Rain.data)[, contVar], xlab = "",
      upper.panel = panel.hexbinplot,
      pscales = 0, xbins = 20,
      varnames = contVar,
      lower.panel = cor.print)
```

`Year` is almost uncorrelated with both `Rain` and `SOI`. There is a weak positive correlation between `Rain` and `SOI`. To investigate the relation between `Rain` and `Phase` we consider the distribution of `Rain` stratified by `Phase`:

```{r warning = FALSE, echo = FALSE, fig.width = 4, fig.height = 4, fig.align = 'center'}
facVar <- c("Phase")
mRain <- melt(Rain.data[, c("Rain", facVar)],
              id = "Rain")

ggplot(mRain,
       aes(x = factor(value, levels = 1:5), y = Rain)) +
  geom_boxplot(fill = I(gray(0.8))) + xlab("") +
  facet_wrap(~ variable, scale = "free_x", ncol = 2) + theme_bw()
```

The boxplot suggests that the rainfall is larger in phase 2 and 4, while it appears particularly low in phase 1 and 2. The plot display some correlation between `Phase` and `Rain` as there appear to be a difference in rainfall within the different phases. We finally investigate the correlation between `SOI` and `Phase`. Since both variables describe aspects of SOI we expect some correlation between them.

```{r}
pander(summary(lm(SOI ~ Phase, Rain.data)))
```

The simple linear model uncovers a high adjusted $R^2$ implying a high correlation between the two variables. All coefficients except `Phase3` are significant indicating that the `Phase` variable is a good predictor of `SOI`. This discovery is further supported by the following plot

```{r warning = FALSE, echo = FALSE, fig.width = 4, fig.height = 4, fig.align = 'center'}
facVar <- c("Phase")
mSOI <- melt(Rain.data[, c("SOI", facVar)],
              id = "SOI")

ggplot(mSOI,
       aes(x = factor(value, levels = 1:5), y = SOI)) +
  geom_boxplot(fill = I(gray(0.8))) + xlab("") +
  facet_wrap(~ variable, scale = "free_x", ncol = 2) + theme_bw()
```

Since the two predictors are collinear to some extent we will be careful, when we consider models with both covariates. It is worth noting though, that while `SOI` is a numeric variable containing exact values of SOI, `Phase` contains information on the 'direction' of the SOI, i.e. whether it is increasing, decreasing or constant. So although the two variables are correlated they both contain information that the other does not, so it could still be valuable to include both variables in a model. We will go into further details on this matter in subsequent sections.

We finally investigate the correlation between `Year` and `Phase`. Since `Phase` is categorical and `Year` is ordinal we use a boxplot to plot the two variables against each other

```{r warning = FALSE, echo = FALSE, fig.width = 4, fig.height = 4, fig.align = 'center'}
mYear <- melt(Rain.data[, c("Year", facVar)],
              id = "Year")

ggplot(mYear,
       aes(x = factor(value, levels = 1:5), y = Year)) +
  geom_boxplot(fill = I(gray(0.8))) + xlab("") +
  facet_wrap(~ variable, scale = "free_x", ncol = 2) + theme_bw()
```
The plot does not suggest any dependence between `Year` and `Phase`. 

## Missing data
With a better understanding of the data we proceed to investigate and handle the missing data. As noted previously the only missing data is in the response variable `Rain` (7.5 pct. missing).

```{r}
pander(Rain.data %>% dplyr::filter(is.na(Rain)))
```

We plot the marginal distribution of the variables with missing data to obtain a visual understanding of patterns in the missing data:

```{r warning = FALSE, echo = FALSE, fig.width = 5.5, fig.height = 3, fig.align = 'center'}
tmp_missing <- lapply(names(Rain.data), function(x)
  ggplot(data = (Rain.data %>% dplyr::filter(is.na(Rain)))[, x, drop = FALSE]) +
    aes_string(x) +
    xlab(x) + 
    ylab("") +
    theme_bw()
  )

grid.arrange(
  tmp_missing[[4]] + gd,
  tmp_missing[[5]] + gb,
  ncol = 2
)
```

Given the modest number of missing observations, it is difficult to identify any clear trends in the missingness. We note that the missingness occurs in consecutive years (except 1933), but there are no distinct pattern beyond that. Without metadata and additional knowledge we are unable to determine if the data is missing completely at random (MCAR), at random (MAR) or not at random (MNAR). It seems unlikely that observations of large rainfall were selectively deleted or that someone was too lazy to record rainfall during heavy rainfalls. A more plausible explanation, given the consecutive years, is that the equipment may be broken for consecutive years or there was a lack of funding these years. The latter explanations would imply that the data is MCAR which implies MAR and we therefore choose to adopt this assumption.

Assuming MAR we can use multiple imputation to impute the missing data. If the MAR assumption holds the imputed values are unbiased and the variation in the data set is preserved. We use the `mice()` function (multiple imputations using chained equations) from the package `mice` to perform the multiple imputations. This procedure uses random draws from the conditional distribution of the target variable given the other variables. That is, we take a bootstrap sample from our data and fit a regression model to this sample to predict the missing values. We use _predictive mean matching_ (PMM) to replace the missing values in each imputed data set. This method uses the value of a donor observation to fill in the missing values. The donors are identified by matching the predicted value of the target to the donor value. PMM does not require any distributional assumptions and is therefore fairly robust to different types of data (F. E. Harell, Jr., 2015). 

We run the imputations and display the imputed values for each data set:

```{r warning = FALSE}
Rain.data.impute <- mice(Rain.data, method = "pmm", 
                         m = 5, seed = 10102024,  
                         printFlag = FALSE)

missing_rain_imp <- Rain.data.impute$imp$Rain
missing_years <- Rain.data$Year[as.numeric(rownames(missing_rain_imp))]
pander(data.frame(Year = missing_years, Rain = missing_rain_imp, row.names = NULL))
```

We plot the density of the five imputed data sets (red dotted) along with the density of the original data set (black solid) to check if the imputed data sets resemble each other and the original data set.

```{r warning = FALSE, echo = FALSE, fig.width = 5, fig.height = 4, fig.align = 'center'}
ggplot(data = Rain.data, aes(x = Rain)) +
  geom_density(size = 1) +
  ggtitle("Density plot of original data and imputed data sets") +
  geom_density(data = complete(Rain.data.impute, 1), 
               aes(x = Rain), color = "red", size = 0.5, lty = 2) +
  geom_density(data = complete(Rain.data.impute, 2), 
               aes(x = Rain), color = "red", size = 0.5, lty = 2) +
  geom_density(data = complete(Rain.data.impute, 3), 
               aes(x = Rain), color = "red", size = 0.5, lty = 2) +
  geom_density(data = complete(Rain.data.impute, 4), 
               aes(x = Rain), color = "red", size = 0.5, lty = 2) +
  geom_density(data = complete(Rain.data.impute, 5), 
               aes(x = Rain), color = "red", size = 0.5, lty = 2) +
  ylab("Density") +
  theme_bw()
```

We note that the densities for all the imputed data sets look very similar to the original data set. We therefore proceed with the imputed data sets.


# Analysis using SOI phase
In the following part of the project we seek to fit the Tweedie exponential dispersion model to the Eromanga rain data, and predict rainfall as a function of the SOI phase. To fit a Tweedie model we need to estimate the nuisance parameter $k$ assumed to be between 1 and 2. 


## Estimating k with a linear regression model
We initially try to estimate $k$ using the linear relation $VY = \psi \mathcal{V}(\mu) = \psi \mu^k$. This implies that $\log (VY) = \log(\psi) + k \log(\mu)$ and we can therefore estimate $k$ by a linear regression of $\log (VY)$ on $\log(\mu)$. We compute the empirical variance and mean of the `Rain` variable within each SOI phase for each imputed data set and display the result for the first imputed data set:

```{r}
grouped_imputatations <- list()
for (i in 1:5) {
  grouped_imputatations[[i]] <- complete(Rain.data.impute, i) %>% group_by(Phase) %>% 
    summarise(meanY = mean(Rain), varY = var(Rain))
}
pander(grouped_imputatations[[1]])
```

Using the empirical mean and variance of $Y$ in each SOI phase we proceed to fit an additive linear regression to estimate $k$ and the dispersion parameter for each imputed data set:

```{r}
lm.fit.imputed <- list()
for (i in 1:5) {
  lm.fit.imputed[[i]] <- lm(log(varY) ~ log(meanY), data = grouped_imputatations[[i]])
}
pander(lm.fit.imputed)
```


We get an initial estimate of $k$ between $1.388$ and $1.470$ and $\psi$ between $\exp(1.975) = 7.201$ and $\exp(2.133) = 8.440$. Like the density plots the estimates are fairly similar. We compute confidence interval (based on the standard deviation in the above table output) for the estimates of each imputation and plot them:

```{r warning = FALSE, echo = FALSE, fig.width = 3, fig.height = 3, fig.align = 'center'}
k_hat_lin_conf <- list()
for (i in 1:5) {
  k_hat_lin_confest <- lm.fit.imputed[[i]]$coef[[2]]
  k_hat_lin_confsd <- summary(lm.fit.imputed[[i]])$coef[2,2]
  k_hat_lin_conf[[i]] <- list("Estimate" = k_hat_lin_confest,
                           "LowerCI" = lm.fit.imputed[[i]]$coef[[2]] - 1.96 * summary(lm.fit.imputed[[i]])$coef[2,2],
                           "UpperCI" = lm.fit.imputed[[i]]$coef[[2]] + 1.96 * summary(lm.fit.imputed[[i]])$coef[2,2])
}

estimates_df <- do.call(rbind, lapply(k_hat_lin_conf, as.data.frame))
estimates_df$Imputation <- 1:nrow(estimates_df)

ggplot(estimates_df, aes(x = Imputation, y = Estimate, ymin = LowerCI, ymax = UpperCI)) +
  geom_pointrange() +
  geom_errorbar(aes(ymin = LowerCI, ymax = UpperCI), width = 0.2, color = "black") +
  theme_minimal() +
  ggtitle("Estimates of k for each imputation with confidence interval") +
  labs(x = "Imputation",
       y = "Estimate") +
  theme(plot.title = element_text(size=7)) +
  coord_flip()  # Flip coordinates for a horizontal display
```
As all estimates of $k$ are within the same margin of error (according to these confidence intervals), we choose to merge the imputations into a single data set to simplify future computations and communicate results more clearly. We should however keep in mind that this may artificially reduce the variance of the data and in a more thorough analysis we should keep all five imputed data sets.

We use the `merge_imputations` from the `sjmisc` package which merges multiple imputed data frames from `mice::mids()-objects` into a single data frame by computing the mean or selecting the most likely imputed value.

```{r}
Rain.data.comp <- Rain.data %>% 
  mutate(Rain = merge_imputations(Rain.data, Rain.data.impute)$Rain)

Rain.data.grp <- Rain.data.comp %>% 
  group_by(Phase) %>% 
  summarise(meanY = mean(Rain), varY = var(Rain))

lm.fit <- lm(log(varY) ~ log(meanY), data = Rain.data.grp)
pander(lm.fit)
```

We end up with estimates for $k$ and $\psi$ of

```{r}
k_hat_lin <- lm.fit$coef[[2]]
psi_hat_lin <- exp(lm.fit$coef[[1]])

pander(c("Estimate of k" = k_hat_lin, "Estimate of psi" = psi_hat_lin))
```

We can calculate a 95% confidence interval for the estimates of $k$ and $\log(\psi)$ using a profile likelihood method:

```{r}
pander(confint(lm.fit))
```

We can visually check that the estimated regression coefficients are reasonably estimated by plotting the fitted regression line on top of the data:

```{r echo = FALSE, fig.width = 4, fig.height = 3.5, fig.align = 'center'}
ggplot(data = Rain.data.grp, aes(x = log(meanY), y = log(varY))) +
  geom_point() +
  geom_smooth(method = "lm", formula = y ~ x, se = FALSE, color = "black", size = 0.5) +
  xlab("log(mean(Y))") +
  ylab("log(var(Y))") +
  theme_bw()
```

We see no warning signs from the plot so is seems fair to assume, that the estimated values of $k$ and $\psi$ are decent initial estimates of the true values.

## Fitting a Tweedie model to the data

We proceed to fit a Tweedie model to the data. We use the `tweedie()` family specification from the `tweedie` package to fit the model with the estimated value of $k$ and `link.power = 0` for the log-link.

```{r}
tweedie.fit <- glm(Rain ~ Phase, data = Rain.data.comp, 
                   family = tweedie(var.power = k_hat_lin, link.power = 0))
pander(summary(tweedie.fit))
```
```{r echo = FALSE}
tweedie.fit.pred <- predict(tweedie.fit, type = "response", newdata = data.frame(Phase = as.factor(c(1,2,3,4,5))))
```


To interpret the model output we recall the five SOI levels

+ Phase 1: Consistently negative
+ Phase 2: Consistently positive
+ Phase 3: Rapidly falling
+ Phase 4: Rapidly rising
+ Phase 5: Consistently near zero

Note that Phase 1 is the reference phase (Intercept). The model suggests that the rainfall for Phase 1 is significantly different from 0 with a point estimate of the average rainfall of 
```{r echo = FALSE}
pander(round(tweedie.fit.pred[[1]], 3))
```
millimeters in July. The model estimates that rainfall for Phase 2 is significantly different from the rainfall in Phase 1 with a point estimate of average rainfall of
```{r echo = FALSE}
pander(round(tweedie.fit.pred[[2]], 3))
```
millimeters in July when SOI is in this phase. The model estimates that the rainfall for Phase 3 is insignificantly different from the rainfall in Phase 1 with a point estimate of rainfall of
```{r echo = FALSE}
pander(round(tweedie.fit.pred[[3]], 3))
```
millimeters on average in July. It is worth noting, that we have quite few observations in this group which could distort the model conclusion as it simply does not have enough information to determine whether or not Phase 1 and Phase 3 are significantly different. The rainfall for Phase 4 is significantly different from the rainfall in Phase 1 according to the model with a point estimate of average rainfall of 
```{r echo = FALSE}
pander(round(tweedie.fit.pred[[4]], 3))
```
millimeters in July. Finally, the model estimates rainfall for Phase 5 to be borderline significantly different from rainfall in Phase 1 with a point estimate of average rainfall of 
```{r echo = FALSE}
pander(round(tweedie.fit.pred[[5]], 3))
```
millimeters in July. Hence, the model suggests that Phase 2 and Phase 4, which are when the SOI is consistently positive and rapidly rising respectively, leads to significantly more rain on average than Phase 1. Phase 5, when the SOI is consistently near zero, is predicted to have slightly more rainfall on average, compared to Phase 1 while the model predicts the lowest rainfall on average in SOI Phases 1 and 3 with no significant difference between them. 

Furthermore, the model predicts $\psi$ to be
```{r echo = FALSE}
psi_hat_tweedie.fit <- summary(tweedie.fit)$dispersion
pander(psi_hat_tweedie.fit)
```
which is slightly different from the estimate
```{r echo = FALSE}
pander(psi_hat_lin)
```
obtained from the linear regression.

## Estimating probability of zero rain in July
We use the estimates obtained in the previous exercises to estimate the probability that it will not rain in July. In the theoretical exercises we derived the probability of zero rain to be

$$ \mathbb{P}(Y = 0) = \exp(-\lambda^*) = \exp(-\frac{\mu^{2-k}}{\psi (2-k)}) $$

With the two estimates of $\psi$ from the previous exercise, we compute two estimates of the the probability that it will not rain in July. We plug in the estimated values of $k$ and the empirical mean of our data:

```{r}
mu_hat <- mean(Rain.data.comp$Rain)

pander(c("Estimate using psi from linear model" 
         = exp(-mu_hat^(2 - k_hat_lin)/(psi_hat_lin*(2 - k_hat_lin))), 
         "Estimate using psi from Tweedie model" 
         = exp(-mu_hat^(2 - k_hat_lin)/(psi_hat_tweedie.fit*(2 - k_hat_lin)))))
```
The two results are quite similar and compared to the empirical probability of zero rain in July
```{r}
pander(sum(Rain.data.comp$Rain == 0)/nrow(Rain.data.comp))
```
we obtain three estimates that are all very similar. 


## Determining k by minimizing AIC
We now estimate $k$ by minimizing the Akaike Information Criterion (AIC) with a profile likelihood of a model with SOI phase as explanatory variable. That is, we search for the value of $k \in (1,2)$ that minimizes the AIC. We start by constructing a general profile likelihood function that takes inputs a formula, a family, a data set and an evaluation metric that we wish to optimize.

```{r}
profile_likelihood <- function(form, family, data, eval) {
  model <- glm(form, 
               family = family, 
               data = data)
  eval_val <- eval(model)
  return(eval_val)
}
```

We define the specific profile likelihood that minimizes the AIC for different values of $k$ of a Tweedie exponential model with rainfall as response and SOI phase as covariate.  That is, we specify `form = Rain ~ Phase`, `family = tweedie(var.power = k, link.power = 0)`, `data = Rain.data.comp` and `eval = AICtweedie`:

```{r}
tweedie.AIC_profile_likelihood <- function(k) {
  profile_likelihood(form = Rain ~ Phase, 
                         family = tweedie(var.power = k, link.power = 0), 
                         data = Rain.data.comp,
                         eval = AICtweedie)
}
```

We use `optimize()` to minimize the AIC and find the optimal value of $k$. We only check values of $k$ between $1.05$ and $1.95$ to avoid numerical instability:

```{r}
k_hat_AIC <- optimize(tweedie.AIC_profile_likelihood, lower = 1.05, upper = 1.95)$minimum
pander(k_hat_AIC)
```

We note that the optimal value of $k$ using the AIC profile likelihood method is fairly close to the value of $k$ estimated by the linear model, which is
```{r echo = FALSE}
pander(k_hat_lin)
```
. To ensure, that we have found a minimum for $1<k<2$ we plot the AIC against $k$ and add a vertical and a horizontal line at the optimal value of $k$.

```{r echo = FALSE, warning = FALSE, message = FALSE, fig.width = 4, fig.height = 4, fig.align = 'center'}
k_plot_df <- data.frame(k = seq(1.05, 1.95, by = 0.001), AIC = sapply(seq(1.05, 1.95, by = 0.001), FUN = tweedie.AIC_profile_likelihood))

k_hat_AIC_obj <- optimize(tweedie.AIC_profile_likelihood, lower = 1.05, upper = 1.95)$objective

ggplot(data = k_plot_df, aes(x = k, y = AIC)) +
  geom_line() +
  xlab("k") +
  ylab("AIC") +
  ggtitle("AIC against k") +
  theme_bw() +
  geom_vline(xintercept = k_hat_AIC, linetype = "dashed") +
  geom_hline(yintercept = k_hat_AIC_obj, linetype = "dashed") +
  geom_text(aes(x = k_hat_AIC + 0.01, y = 850, label = paste0("Optimal value of k at ", round(k_hat_AIC,2))),
            hjust = 0, vjust = 0, size = 2.5) +
  geom_text(aes(x = 1.83, y = k_hat_AIC_obj + 20, label = paste0("Minimum AIC at ", round(k_hat_AIC_obj,2))), size = 2.5)
```
The plot confirms, that `optimize()` has found the global minimum of the AIC for $1<k<2$. We repeat the calculations from the previous exercises using the optimal value of $k$ found by minimizing the profile likelihood.


## Re-estimating with new value of k
As before we initially fit a Tweedie model to the data using the optimal value of $k$ found by minimizing the AIC profile likelihood.

```{r}
tweedie.fit.AIC <- glm(Rain ~ Phase, data = Rain.data.comp, 
                       family = tweedie(var.power = k_hat_AIC, link.power = 0))
pander(summary(tweedie.fit.AIC))
```

We note a slight decrease in standard error and p-value for all coefficients and the estimated dispersion parameter
```{r echo = FALSE}
psi_hat_model_AIC <- summary(tweedie.fit.AIC)$dispersion
pander(psi_hat_model_AIC)
```
is slightly different from the previous estimate 
```{r echo = FALSE}
pander(psi_hat_tweedie.fit)
```
of the Tweedie model. Apart from that the results are identical to the previous model and the interpretation is the same.

We recalculate the estimated probability that it will not rain in July by plugging in the estimated values of $k$ and $\psi$ and the empirical mean of our data:
```{r}
pander(c("Estimate using psi from linear model" 
         = exp(-mu_hat^(2 - k_hat_AIC)/(psi_hat_lin*(2 - k_hat_AIC))), 
         "Estimate using psi from Tweedie model" 
         = exp(-mu_hat^(2 - k_hat_AIC)/(psi_hat_model_AIC*(2 - k_hat_AIC)))))

```

Again the estimates are similar to the estimates obtained from the previous estimate of $k$.


## Model diagnostics
We check the model assumptions for the two Tweedie models fitted in the previous exercise. First we construct a data frame with the relevant diagnostic information for both Tweedie models. We extract the fitted values, Pearson residuals and deviance residuals for both models and add the SOI phase as a variable to the data frame.

```{r echo = FALSE}
linmod_diag <- fortify(tweedie.fit)
AIC_diag <- fortify(tweedie.fit.AIC)

diag_df <- data.frame(linmod.fitted = linmod_diag$.fitted,
                      linmod.pearson = residuals(tweedie.fit, type = "pearson"),
                      linmod.deviance = residuals(tweedie.fit, type = "deviance"),
                      AIC.fitted = AIC_diag$.fitted,
                      AIC.pearson = residuals(tweedie.fit.AIC, type = "pearson"),
                      AIC.deviance = residuals(tweedie.fit.AIC, type = "deviance"),
                      SOI = Rain.data.comp$SOI)

pander(summary(diag_df))
```

We then plot the residuals against the fitted values, first for the linear model estimate of $k$:

```{r, warning=FALSE, echo = FALSE, message = FALSE, fig.width = 10, fig.height = 6, fig.align = 'center'}
diag_lin_DR_fitted <- qplot(x = linmod.fitted, y = linmod.deviance, data = diag_df) +
  geom_point() +
  geom_smooth() +
  xlab("Fitted values") +
  ylab("Deviance residuals") +
  ggtitle("Deviance residuals against fitted values") +
  theme_bw() +
  theme(plot.title=element_text(size=10))

diag_lin_PR_fitted <- qplot(x = linmod.fitted, y = linmod.pearson, data = diag_df) +
  geom_point() +
  geom_smooth() +
  xlab("Fitted values") +
  ylab("Pearson residuals") +
  ggtitle("Pearson residuals against fitted values") +
  theme_bw() +
  theme(plot.title=element_text(size=10))

diag_lin_DR_SOI <- qplot(x = SOI, y = linmod.deviance, data = diag_df) +
  geom_point() +
  geom_smooth() +
  xlab("SOI") +
  ylab("Deviance residuals") +
  ggtitle("Deviance residuals against SOI") +
  theme_bw() +
  theme(plot.title=element_text(size=10))

diag_lin_PR_SOI <- qplot(x = SOI, y = linmod.pearson, data = diag_df) +
  geom_point() +
  geom_smooth(size = 1) +
  xlab("SOI") +
  ylab("Pearson residuals") +
  ggtitle("Pearson residuals against SOI") +
  theme_bw() +
  theme(plot.title=element_text(size=10))

grid.arrange(diag_lin_DR_fitted, diag_lin_PR_fitted, diag_lin_DR_SOI, diag_lin_PR_SOI, ncol = 2,
             top = textGrob(paste0("k = ", round(k_hat_lin, 3))))
```

From the plots there is no clear indication of model misspecification. The residuals appear to be randomly scattered around zero, indicating that the model captures the mean and variance structure of the data. We further evaluate the plots with bootstrapping. That is, using the estimated mean, dispersion parameter and $k$ we simulate data from a Tweedie distribution and fit a new Tweedie model to this data. We then plot the residuals against the fitted values. If the original data is from a Tweedie distribution with the estimated parameters, we should observe, that the bootstrapped residual plots resemble the original residual plots.

```{r warning=FALSE, echo = FALSE, message = FALSE, fig.width = 10, fig.height = 6, fig.align = 'center'}
set.seed(10102024)

# Retrieve the estimated mean value of Y given the covariates
linmu_hats <- predict(tweedie.fit, type = "response")

# Use the estimated mean value of Y given the covariates, the estimate of k and the estimated dispersion parameter to simulate new data
yNew1 <- rTweedie(linmu_hats, psi_hat_tweedie.fit, p = k_hat_lin)
yNew2 <- rTweedie(linmu_hats, psi_hat_tweedie.fit, p = k_hat_lin)
yNew3 <- rTweedie(linmu_hats, psi_hat_tweedie.fit, p = k_hat_lin)
yNew4 <- rTweedie(linmu_hats, psi_hat_tweedie.fit, p = k_hat_lin)
yNew5 <- rTweedie(linmu_hats, psi_hat_tweedie.fit, p = k_hat_lin)
yNew6 <- rTweedie(linmu_hats, psi_hat_tweedie.fit, p = k_hat_lin)
simGlmNew1 <- glm(yNew1 ~ Rain.data.comp$Phase, tweedie(var.power = k_hat_lin, link.power = 0))
simGlmNew2 <- glm(yNew2 ~ Rain.data.comp$Phase, tweedie(var.power = k_hat_lin, link.power = 0))
simGlmNew3 <- glm(yNew3 ~ Rain.data.comp$Phase, tweedie(var.power = k_hat_lin, link.power = 0))
simGlmNew4 <- glm(yNew4 ~ Rain.data.comp$Phase, tweedie(var.power = k_hat_lin, link.power = 0))
simGlmNew5 <- glm(yNew5 ~ Rain.data.comp$Phase, tweedie(var.power = k_hat_lin, link.power = 0))
simGlmNew6 <- glm(yNew6 ~ Rain.data.comp$Phase, tweedie(var.power = k_hat_lin, link.power = 0))
simDiagNew1 <- fortify(simGlmNew1)
simDiagNew2 <- fortify(simGlmNew2)
simDiagNew3 <- fortify(simGlmNew3)
simDiagNew4 <- fortify(simGlmNew4)
simDiagNew5 <- fortify(simGlmNew5)
simDiagNew6 <- fortify(simGlmNew6)
p1 <- qplot(.fitted, .resid, data = simDiagNew1) + xlab("Fitted values") +
  ylab("Deviance residuals") + geom_smooth() + theme_bw()
p2 <- qplot(.fitted, .resid, data = simDiagNew2) + xlab("Fitted values") +
  ylab("Deviance residuals") + geom_smooth() + theme_bw()
p3 <- qplot(.fitted, .resid, data = simDiagNew3) + xlab("Fitted values") +
  ylab("Deviance residuals") + geom_smooth() + theme_bw()
p4 <- qplot(.fitted, .resid, data = simDiagNew4) + xlab("Fitted values") +
  ylab("Deviance residuals") + geom_smooth() + theme_bw()
p5 <- qplot(.fitted, .resid, data = simDiagNew5) + xlab("Fitted values") +
  ylab("Deviance residuals") + geom_smooth() + theme_bw()
p6 <- qplot(.fitted, .resid, data = simDiagNew6) + xlab("Fitted values") +
  ylab("Deviance residuals") + geom_smooth() + theme_bw()

grid.arrange(p1, p2, p3, p4, p5, p6, ncol = 3,
             top = textGrob("Bootstrap deviance residual plots"))
```
We see that the plots by and large resemble the residual plots of the original model. This supports the initial diagnostic plots. That is, there is no clear evidence that the model assumptions are violated. We repeat the procedure for the model fitted with the AIC profile likelihood estimated $k$.

```{r, warning=FALSE, echo = FALSE, message = FALSE, fig.width = 10, fig.height = 6, fig.align = 'center'}
diag_AIC_DR_fitted <- qplot(x = AIC.fitted, y = AIC.deviance, data = diag_df) +
  geom_point() +
  geom_smooth() +
  xlab("Fitted values") +
  ylab("Deviance residuals") +
  ggtitle("Deviance residuals against fitted values") +
  theme_bw() +
  theme(plot.title=element_text(size=10))


diag_AIC_PR_fitted <- qplot(x = AIC.fitted, y = AIC.pearson, data = diag_df) +
  geom_point() +
  geom_smooth() +
  xlab("Fitted values") +
  ylab("Pearson residuals") +
  ggtitle("Pearson residuals against fitted values") +
  theme_bw() +
  theme(plot.title=element_text(size=10))

diag_AIC_DR_SOI <- qplot(x = SOI, y = AIC.deviance, data = diag_df) +
  geom_point() +
  geom_smooth() +
  xlab("SOI") +
  ylab("Deviance residuals") +
  ggtitle("Deviance residuals against SOI") +
  theme_bw() +
  theme(plot.title=element_text(size=10))

diag_AIC_PR_SOI <- qplot(x = SOI, y = AIC.pearson, data = diag_df) +
  geom_point() +
  geom_smooth(size = 1) +
  xlab("SOI") +
  ylab("Pearson residuals") +
  ggtitle("Pearson residuals against SOI") +
  theme_bw() +
  theme(plot.title=element_text(size=10))

grid.arrange(diag_AIC_DR_fitted, diag_AIC_PR_fitted, diag_AIC_DR_SOI, diag_AIC_PR_SOI, ncol = 2,
             top = textGrob(paste0("k = ", round(k_hat_AIC, 3))))
```

We notice a pattern similar to the previous model and the conclusion is the same.

```{r warning=FALSE, echo = FALSE, message = FALSE, fig.width = 10, fig.height = 6, fig.align = 'center'}
set.seed(10102024)

AICmu_hats <- predict(tweedie.fit.AIC, type = "response")

AICyNew1 <- rTweedie(AICmu_hats, psi_hat_model_AIC, p = k_hat_AIC)
AICyNew2 <- rTweedie(AICmu_hats, psi_hat_model_AIC, p = k_hat_AIC)
AICyNew3 <- rTweedie(AICmu_hats, psi_hat_model_AIC, p = k_hat_AIC)
AICyNew4 <- rTweedie(AICmu_hats, psi_hat_model_AIC, p = k_hat_AIC)
AICyNew5 <- rTweedie(AICmu_hats, psi_hat_model_AIC, p = k_hat_AIC)
AICyNew6 <- rTweedie(AICmu_hats, psi_hat_model_AIC, p = k_hat_AIC)
AICsimGlmNew1 <- glm(AICyNew1 ~ Rain.data.comp$Phase, tweedie(var.power = k_hat_AIC, link.power = 0))
AICsimGlmNew2 <- glm(AICyNew2 ~ Rain.data.comp$Phase, tweedie(var.power = k_hat_AIC, link.power = 0))
AICsimGlmNew3 <- glm(AICyNew3 ~ Rain.data.comp$Phase, tweedie(var.power = k_hat_AIC, link.power = 0))
AICsimGlmNew4 <- glm(AICyNew4 ~ Rain.data.comp$Phase, tweedie(var.power = k_hat_AIC, link.power = 0))
AICsimGlmNew5 <- glm(AICyNew5 ~ Rain.data.comp$Phase, tweedie(var.power = k_hat_AIC, link.power = 0))
AICsimGlmNew6 <- glm(AICyNew6 ~ Rain.data.comp$Phase, tweedie(var.power = k_hat_AIC, link.power = 0))
AICsimDiagNew1 <- fortify(AICsimGlmNew1)
AICsimDiagNew2 <- fortify(AICsimGlmNew2)
AICsimDiagNew3 <- fortify(AICsimGlmNew3)
AICsimDiagNew4 <- fortify(AICsimGlmNew4)
AICsimDiagNew5 <- fortify(AICsimGlmNew5)
AICsimDiagNew6 <- fortify(AICsimGlmNew6)
AICp1 <- qplot(.fitted, .resid, data = AICsimDiagNew1) + xlab("Fitted values") +
  ylab("Deviance residuals") + geom_smooth() + theme_bw()
AICp2 <- qplot(.fitted, .resid, data = AICsimDiagNew2) + xlab("Fitted values") +
  ylab("Deviance residuals") + geom_smooth() + theme_bw()
AICp3 <- qplot(.fitted, .resid, data = AICsimDiagNew3) + xlab("Fitted values") +
  ylab("Deviance residuals") + geom_smooth() + theme_bw()
AICp4 <- qplot(.fitted, .resid, data = AICsimDiagNew4) + xlab("Fitted values") +
  ylab("Deviance residuals") + geom_smooth() + theme_bw()
AICp5 <- qplot(.fitted, .resid, data = AICsimDiagNew5) + xlab("Fitted values") +
  ylab("Deviance residuals") + geom_smooth() + theme_bw()
AICp6 <- qplot(.fitted, .resid, data = AICsimDiagNew6) + xlab("Fitted values") +
  ylab("Deviance residuals") + geom_smooth() + theme_bw()

grid.arrange(AICp1, AICp2, AICp3, AICp4, AICp5, AICp6, ncol = 3,
             top = textGrob("Bootstrap deviance residual plots"))
```

Again, the pattern appears to be similar to the previous model. There may be concerns with the second and fifth plot where the residual plots diverge slightly from the original plots. It is difficult to say however if this is just noise or if the first estimate of $k$ is better than the second.

## Conclusion
The analysis indicates an association between rainfall and SOI phase, a relationship initially suggested by boxplot of rainfall and SOI phase in the EDA and later supported by the models in the subsequent analysis. Based on the model diagnostics, both fitted models seem to adequately capture the mean and variance structure of the data, allowing us to reasonably trust the model conclusions. The various estimates of $k$ and $\psi$ were all quite close and considered the profile-likelihood confidence interval of $k$ and $\log(\psi)$ all estimates were within a statistical margin of error.

It therefore appears, that using SOI phase as the only predictor of rainfall is reasonable approach. However, as mentioned there could be an issue with the few observations in Phase 3, which makes it difficult to estimate the effect of this phase. Therefore it may be beneficial to construct a model using `SOI` instead of `Phase`, which we will look into in a coming part of the analysis.

## Bootstrap estimates of k

We conclude this part of the analysis with parametric bootstrap to estimate the sampling distribution of $k$. Using the AIC-based estimate of $k$ and the parameters estimated by the Tweedie model associated to this estimate of $k$, we simulate data from a Tweedie distribution and fit a new Tweedie model to the simulated data. We repeat this process multiple times to estimate the sampling distribution of $k$. To carry out the parametric bootstrap we need the strong distributional assumptions GA3 and A5. We have not formally tested these assumptions, so further analysis should check their validity. For now we acknowledge, that the parametric bootstrap may produce too narrow confidence intervals.

Due to the small number of observations in Phase 3 there is a high probability, that some bootstrap samples will contain only zeros in this phase. This implies, that the fitted Tweedie model used in the profile-likelihood will fail to converge. To mitigate this issue, we add a small random value to the observations in the group, in case they are all zeros. While this method introduces some bias, we prefer this method over relying on estimates from a non-converged algorithm which are meaningless.

```{r}
set.seed(10102024)
B <- 1000
parametric_k_AIC <- numeric(B)
rain_data_AIC_sample <- Rain.data.comp


for (b in 1:B){
  rain_data_AIC_sample$Rain <- rTweedie(AICmu_hats, 
                                        psi_hat_model_AIC, 
                                        p = k_hat_AIC)
  
  
  # Calculate sum for each phase in order to handle zeros
  rain_data_AIC_sample <- rain_data_AIC_sample %>%
    group_by(Phase) %>%
    mutate(phase_sum = sum(Rain)) %>%
    ungroup()

  # Add small constant to phases where sum of Rain is zero to ensure convergence 
  rain_data_AIC_sample <- rain_data_AIC_sample %>%
    mutate(Rain = if_else(phase_sum == 0,
                          Rain + abs(rnorm(n(), 0.0001, 0.001)),
                          Rain))

  tweedieBootstrap.AIC_profile_likelihood <- function(k) {
    profile_likelihood(form = Rain ~ Phase, 
                       family = tweedie(var.power = k, link.power = 0), 
                       data = rain_data_AIC_sample,
                       eval = AICtweedie)
  }
  
  boot_k_hat <- optimize(tweedieBootstrap.AIC_profile_likelihood, 
                         lower = 1.1, upper = 1.9)$minimum
  
  parametric_k_AIC[b] <- boot_k_hat
}
```



```{r warning=FALSE, echo = FALSE, message = FALSE, fig.width = 10, fig.height = 6, fig.align = 'center'}
violin_df <- data.frame(k = c(parametric_k_AIC),
                        method = as.factor(rep(c("AIC"), each = B)))

ggplot(violin_df, aes(x = method, y = k)) +
  geom_violin(fill = I(gray(0.8))) + xlab("") +
  facet_wrap(~ method, scale = "free_x", ncol = 2) + theme_bw() +
  geom_hline(yintercept = k_hat_AIC, color = "red", linetype = "dashed") +
  annotate("text", x = 1, y = k_hat_AIC, 
           label = "AIC estimate of k", color = "red", vjust = -0.5)
```

It appears that the AIC method is fairly robust to changes in the bootstrapped samples with the exception of the spike in the plot that comes from the samples where Phase 3 has only zero observations. We use the parametric bootstrap to estimate the standard error of the AIC estimates of $k$. The standard deviation is estimated to be

```{r}
se_AIC <- sd(parametric_k_AIC)
pander(se_AIC)
```
and the lower and upper bound of the 95% confidence interval are
```{r}
pander(k_hat_AIC + c(-1, 1) * 1.96 * se_AIC)
```
respectively. We note that this confidence interval is slightly narrower than the one obtained from the profile likelihood method of the $k$ estimated by the linear model. Both confidence intervals contain both estimates of $k$. We may therefore conclude that the two estimates of $k$ are not significantly different.

\newpage

# Analysis using SOI directly

We choose to model the rainfall conditionally on having rained to provide a different perspective on the data and avoid repeating the previous analysis. We filter out observations where the rainfall is zero:

```{r warning = FALSE, echo = TRUE, message = FALSE}
rain.data1 <- Rain.data.comp %>%
  filter(Rain != 0)
```

We look at the distribution of `Rain`.

```{r warning = FALSE, echo = FALSE, message = FALSE, fig.width = 10, fig.height = 6, fig.align = 'center'}
ggplot(data = rain.data1, aes(x = Rain)) +
  geom_density(fill = gray(0.5)) +
  xlab("Rain") +
  ylab("Density") +
  ggtitle("Rainfall distribution")
  
```

The distribution remains highly right skewed. Consider now `Rain` plotted against `SOI`. 

```{r warning = FALSE, echo = FALSE, message = FALSE, fig.width = 10, fig.height = 6, fig.align = 'center'}
# Plot rain against year using ggplot
ggplot(data = rain.data1, aes(x = SOI, y = Rain)) +
  geom_point() +
  geom_smooth() +
  xlab("SOI") +
  ylab("Rain") + 
  ggtitle("Rain against SOI")

```
There seems to be a positive relationship between `SOI` and `Rain`. The scatter plot also hints at potential non-linear trends and there are a few observations with very large values of `Rain`, but none of which appear extreme enough to be considered outliers.

The above plots motivates the use of a Gamma exponential dispersion model, which is typically used to fit positive continuous right skewed data. The Gamma exponential dispersion model has a quadratic mean variance relationship ($k = 2$), which also seems like a good fit from the scatter plot. We can further support this claim, by estimating the value of $k$ for a Tweedie model with `SOI` as predictor, using the data containing observations with rainfall. We plot the estimated AIC profile likelihood as a function of $k$:

```{r echo = FALSE, warning = FALSE, message = FALSE, fig.width = 4, fig.height = 4, fig.align = 'center'}
tweedie.AIC_profile_likelihood_SOI <- function(k) {
  profile_likelihood(form = Rain ~ SOI, 
                     family = tweedie(var.power = k, link.power = 0), 
                     data = rain.data1,
                     eval = AICtweedie)
}
k_hat_AIC_SOI <- optimize(tweedie.AIC_profile_likelihood_SOI, lower = 1, upper = 2.5)$minimum

k_plot_SOI_df <- data.frame(k = seq(1.9, 2.5, by = 0.001), AIC = sapply(seq(1.9, 2.5, by = 0.001), FUN = tweedie.AIC_profile_likelihood_SOI))

k_hat_AIC_SOI_obj <- optimize(tweedie.AIC_profile_likelihood_SOI, lower = 1.9, upper = 2.5)$objective

ggplot(data = k_plot_SOI_df, aes(x = k, y = AIC)) +
  geom_line() +
  xlab("k") +
  ylab("AIC") +
  ggtitle("AIC against k") +
  theme_bw() +
  geom_vline(xintercept = k_hat_AIC_SOI, linetype = "dashed") +
  geom_hline(yintercept = k_hat_AIC_SOI_obj, linetype = "dashed") +
  geom_text(aes(x = k_hat_AIC_SOI - 0.1, y = 638.5, label = paste0("Optimal value of k at ", round(k_hat_AIC_SOI,3))),
            hjust = 0, vjust = 0, size = 2.5) +
  geom_text(aes(x = 2.4, y = k_hat_AIC_SOI_obj + 0.1, label = paste0("Minimum AIC at ", round(k_hat_AIC_SOI_obj,2))), size = 2.5)
```
The estimate of $k = 2.198$ suggest that the Gamma model could be a reasonable fit. This method of estimating $k$ is of course only valid if the Tweedie describes the data well. 

A typical link function choice for the Gamma model is the log link function. If we fit the model using the log link function, we fit the log of the mean of the response variables as a linear combination of the predictors. Other options include the identity link and the canonical link, which is the inverse function. With the identity link it is likely that the model predictions lie outside the support of the gamma-model, i.e. are negative, which is not reasonable for rainfall and can cause problems if we bootstrap the model. For this reason the identity link is disregarded. We therefore fit two models: One with log link and one using the canonical link. 

```{r}
glm_gamma_log <- glm(Rain ~ SOI, data = rain.data1, family = Gamma("log"))
glm_gamma_inv <- glm(Rain ~ SOI, data = rain.data1, family = Gamma)
```

Since the models have the same complexity, we can compare them using the training error. The training error based on the squared deviance loss function for the two models is

```{r warning = FALSE, echo = FALSE, message = FALSE, fig.width = 10, fig.height = 6, fig.align = 'center'}
tibble(
  "Link function" = c("Log", "Inverse"),
  "Training error" = c(mean(residuals(glm_gamma_log, type = "deviance")^2), 
                       mean(residuals(glm_gamma_inv, type = "deviance")^2))
) %>% pander()
```

The log link function seems to be the best fit in terms of training error. We plot the model fits

```{r warning = FALSE, echo = FALSE, message = FALSE, fig.width = 10, fig.height = 6, fig.align = 'center'}
log_pred <- predict(glm_gamma_log, newdata = rain.data1, type = "response")
inv_pred <- predict(glm_gamma_inv, newdata = rain.data1, type = "response")

grid.arrange(
  qplot(rain.data1$SOI, rain.data1$Rain) + 
    geom_line(aes(y = log_pred), color = "red") +
    xlab("SOI") +
    ylab("Rain")+
    ggtitle("Log link function"),
   qplot(rain.data1$SOI, rain.data1$Rain) + 
    geom_line(aes(y = inv_pred), color = "red") +
    xlab("SOI") +
    ylab("Rain") +
    ggtitle("Inverse link function"),
  nrow = 1
)
```

Both models seem to fit the data reasonably well. There is no evidence of overfitting so we assume the training error would generalize well. Based on this, we choose to proceed with the the log link gamma model rather than the inverse link model.

## Additional predictors

We perform an LRT test to see whether or not we should include additional predictors. We consider to add the predictors `Phase` and `Year.` In the EDA we observed that `Phase` and `SOI` are correlated, and it could be problematic to include both in the model. However, we also discussed that `Phase` contains information that `SOI` does not and vice versa, meaning that the variable could potentially improve our predictions. 

```{r}
add1(glm_gamma_log, Rain ~ SOI + Phase + Year, test = "LRT") %>% kable()
```
According to the LRT test there is not evidence in data that suggest that the additional predictors should be added to the model. We therefore choose to proceed with the model that only includes `SOI` as a predictor. However based on the weak correlation and the considerations regarding `SOI`and `Phase` together with the results of the test it seems very unlikely that including additional predictors will improve the predictive strentgth of the model.

## Nonlinear effects

We consider to include a non-linear effect of `SOI`. In particular we explore the inclusion of a natural cubic spline with 2,3,4,5 or 6 degrees of freedom:

```{r warning = FALSE, echo = TRUE, message = FALSE, fig.width = 10, fig.height = 6, fig.align = 'center'}
form1 <- Rain ~ SOI
form2 <- Rain ~ ns(SOI, df = 2)
form3 <- Rain ~ ns(SOI, df = 3)
form4 <- Rain ~ ns(SOI, df = 4)
form5 <- Rain ~ ns(SOI, df = 5)
form6 <- Rain ~ ns(SOI, df = 6)

glm1 <- glm(form1, data = rain.data1, family = Gamma("log"))
glm2 <- glm(form2, data = rain.data1, family = Gamma("log"))
glm3 <- glm(form3, data = rain.data1, family = Gamma("log"))
glm4 <- glm(form4, data = rain.data1, family = Gamma("log"))
glm5 <- glm(form5, data = rain.data1, family = Gamma("log"))
glm6 <- glm(form6, data = rain.data1, family = Gamma("log"))
```

The model fits: 

```{r warning = FALSE, echo = FALSE, message = FALSE, fig.width = 10, fig.height = 6, fig.align = 'center'}
pred1 <- predict(glm1, newdata = rain.data1, type = "response")
pred2 <- predict(glm2, newdata = rain.data1, type = "response")
pred3 <- predict(glm3, newdata = rain.data1, type = "response")
pred4 <- predict(glm4, newdata = rain.data1, type = "response")
pred5 <- predict(glm5, newdata = rain.data1, type = "response")
pred6 <- predict(glm6, newdata = rain.data1, type = "response")

grid.arrange(
  qplot(rain.data1$SOI, rain.data1$Rain) + 
    geom_line(aes(y = pred1), color = "red") +
    xlab("SOI") +
    ylab("Rain")+
    ggtitle("No expansion"),
   qplot(rain.data1$SOI, rain.data1$Rain) + 
    geom_line(aes(y = pred2), color = "red") +
    xlab("SOI") +
    ylab("Rain") +
    ggtitle("Df = 2"),
   qplot(rain.data1$SOI, rain.data1$Rain) + 
    geom_line(aes(y = pred3), color = "red") +
    xlab("SOI") +
    ylab("Rain") +
    ggtitle("Df = 3"),
  qplot(rain.data1$SOI, rain.data1$Rain) + 
    geom_line(aes(y = pred4), color = "red") +
    xlab("SOI") +
    ylab("Rain") +
    ggtitle("Df = 4"),   
  qplot(rain.data1$SOI, rain.data1$Rain) + 
    geom_line(aes(y = pred5), color = "red") +
    xlab("SOI") +
    ylab("Rain") +
    ggtitle("Df = 5"),
  qplot(rain.data1$SOI, rain.data1$Rain) + 
    geom_line(aes(y = pred6), color = "red") +
    xlab("SOI") +
    ylab("Rain") +
    ggtitle("Df = 6"),
  nrow = 2
)
```

Adding more degrees of freedom to the natural cubic splines adds flexibility to the model, allowing it to fit data better. This comes at the expense of potential overfitting. The model fitted with 6 degress of freedom is quite likely overfitting data. In order to better assess the models in terms of predictions, we do cross-validation to compare the models. We first define the deviance loss, which we will use as error function.

```{r}
# Error function
dev_loss <- function(Y, muhat) 2 * (log(muhat / Y) + Y / muhat - 1)
```

We define the cross validation function. 

```{r}
cv <- function(data, form, B = 1, k = 8, my_family, error_func){
  n <- nrow (data)
  PEcv <- vector("list", B)
  tmp <- numeric(n)
  for (b in 1: B){
    ## Generating the random division into groups
    group <- sample(rep(1:k, length.out = n))
    for (i in 1:k){
      modelcv <- glm(form, data = data[group != i, ], family = my_family)
      muhat <- predict(modelcv, newdata = data[group == i, ], type = "response")
      # !!! change input of error function !!!
      tmp[group == i] <- error_func(data$Rain[group == i],  muhat)
    }
    PEcv[[b]] <- tmp
  }
  mean(unlist(PEcv))
}
```

Since the data set is quite small, we perform LOOCV. This is a non random procedure and we therefore set $B = 1$.

```{r warning = FALSE, echo = FALSE, message = FALSE, fig.width = 10, fig.height = 6, fig.align = 'center'}
tibble("Df."= seq(1,6), 
       "Generalization error" = c(
          cv(data = rain.data1, form = form1, B = 1, k = nrow(rain.data1), 
             my_family = Gamma("log"), 
             error_func = dev_loss),
          cv(data = rain.data1, form = form2, B = 1, k = nrow(rain.data1), 
             my_family = Gamma("log"), 
             error_func = dev_loss),
          cv(data = rain.data1, form = form3, B = 1, k = nrow(rain.data1), 
             my_family = Gamma("log"), 
             error_func = dev_loss),
          cv(data = rain.data1, form = form4, B = 1, k = nrow(rain.data1), 
             my_family = Gamma("log"), 
             error_func = dev_loss),
          cv(data = rain.data1, form = form5, B = 1, k = nrow(rain.data1), 
             my_family = Gamma("log"), 
             error_func = dev_loss),
          cv(data = rain.data1, form = form6, B = 1, k = nrow(rain.data1), 
             my_family = Gamma("log"), 
             error_func = dev_loss))
) %>% pander()
```

We proceed with the model with 2 degrees of freedom as it has the smallest cross validation error. But note that the model without natural cubic splines is very close to performing just as well in terms of generalization error. Since we are interested in prediction, we choose the model with 2 degrees of freedom. If interpretability was of higher priority, we would choose the model without natural cubic splines. 

## Model Diagnostics

We do model diagnostics for the chosen model.

```{r warning = FALSE, echo = FALSE, message = FALSE, fig.width = 10, fig.height = 6, fig.align = 'center'}
mod_final <- glm(form2, data = rain.data1, family = Gamma("log"))

glm_gamma.obj <- transform(
  rain.data1,
  .fitted = predict(mod_final, type = "response"),
  .deviance = residuals(mod_final, type = "deviance"),
  .pearson = residuals(mod_final, type = "pearson")
)

psihat_pplot <- summary(glm_gamma_log)$dispersion
residPPplot <- pgamma(rain.data1$Rain, 
                     shape = 1 / psihat_pplot, 
                     rate = 1  / (psihat_pplot * fitted(glm_gamma_log)))

grid.arrange(
  qplot(.fitted, .deviance, data = glm_gamma.obj) + 
    geom_smooth() + xlab("Fitted values") +
    ylab("Deviance residuals"),
  qplot(.fitted, .pearson, data = glm_gamma.obj) + 
    geom_smooth() + xlab("Fitted values") +
    ylab("Pearson residuals"),
  qplot(.fitted, sqrt(abs(.pearson)), data = glm_gamma.obj) + 
    geom_smooth() + xlab("Fitted values") +
    ylab("Square root of Pearson residuals"),
  qplot(ppoints(length(residPPplot)), sort(residPPplot)) +
    geom_abline(slope = 1, intercept = 0, col = "blue") + 
    xlab("Theoretical uniform quantiles") +
    ylab("Empirical quantiles"),
  ncol = 2
)
```

From the plots we see no clear evidence against the weak model assumptions GA1 and GA2. The residuals seem to be randomly scattered around zero with constant variance. There are a few large residuals, but they are not alarming. As the PP-plot displays, the stronger distributional assumption GA3 seem to be slightly off which implies that inference relying on distributional results should be avoided or employed with consideration. 

We evaluate the residual plots via bootstrapping. We compare the residuals with simulated residuals under the null hypothesis that our model is correct. We repeat the bootstrap procedure 6 times, and plot the deviance residuals against fitted values for each bootstrap sample. 

```{r warning = TRUE, echo = FALSE, message = FALSE, fig.width = 10, fig.height = 6, fig.align = 'center'}
set.seed(1986234)
glm_list <- list()
for(i in 1:6){
  sim_data <- simulate(mod_final)[,1]
  glm_list[[i]] <- glm(sim_data ~ ns(rain.data1$SOI, df = 2),
                       family = Gamma(link = "log"))
}
```

The bootstrapped deviance residual plots 
```{r warning = TRUE, echo = FALSE, message = FALSE, fig.width = 10, fig.height = 6, fig.align = 'center'}
grid.arrange(
  qplot(fitted(glm_list[[1]]), residuals(glm_list[[1]], type = "deviance")) + 
    xlab("Fitted values") +
    ylab("Deviance residuals") +
    geom_smooth(),
  qplot(fitted(glm_list[[2]]), residuals(glm_list[[2]], type = "deviance")) + 
    xlab("Fitted values") +
    ylab("Deviance residuals") +
    geom_smooth(),
  qplot(fitted(glm_list[[3]]), residuals(glm_list[[3]], type = "deviance")) + 
    xlab("Fitted values") +
    ylab("Deviance residuals") +
    geom_smooth(),
  qplot(fitted(glm_list[[4]]), residuals(glm_list[[4]], type = "deviance")) + 
    xlab("Fitted values") +
    ylab("Deviance residuals") +
    geom_smooth(),
   qplot(fitted(glm_list[[5]]), residuals(glm_list[[5]], type = "deviance")) + 
    xlab("Fitted values") +
    ylab("Deviance residuals") +
    geom_smooth(),
  qplot(fitted(glm_list[[6]]), residuals(glm_list[[6]], type = "deviance")) + 
    xlab("Fitted values") +
    ylab("Deviance residuals") +
    geom_smooth(),
  ncol = 3
)
```

The bootstrapped residual plots resemble the original residual plots well. This further indicates that GA1 and GA2 are satisfied. Note how the few large residuals that we observed in the original residual plots are also present in the bootstrapped plots supporting the claim that they are not alarming. The bootstrapped Pearson residual plots lead to the same conclusion and are therefore omitted. 

## Reporting a final model and interpretation

The summary of our final model

```{r warning = FALSE, echo = FALSE, message = FALSE, fig.width = 10, fig.height = 6, fig.align = 'center'}
summary(mod_final) %>% pander()
```

From the summary we can conclude that `SOI` is a significant predictor of `Rain`. Since we have used a natural cubic spline with 2 degrees of freedom to fit our model, the coefficients are difficult to interpret. We instead consider the predictions of the model. We print the model predictions for a few values of SOI:

```{r warning = FALSE, echo = FALSE, message = FALSE, fig.width = 10, fig.height = 6, fig.align = 'center'}
tibble(
  SOI = c(-20,-10,0,10,20),
  "Rain Fall" = predict(mod_final, 
                      newdata = data.frame(SOI = c(-20,-10,0,10,20)), 
                      type = "response")
) %>% pander()
```

A plot of the model fit is shown below.

```{r warning = FALSE, echo = FALSE, message = FALSE, fig.width = 10, fig.height = 6, fig.align = 'center'}
pred2 <- predict(mod_final, newdata = rain.data1, type = "response")

qplot(rain.data1$SOI, rain.data1$Rain) + 
  geom_line(aes(y = pred2), color = "red") +
  xlab("SOI") +
  ylab("Rain") +
  ggtitle("Rain ~ ns(SOI, df = 2)")
```

The fitted model predicts that rainfall is increasing as a function of `SOI.` The slope of the model is largest for values of SOI between $-10$ and $10$. For `SOI` values that are larger or smaller than this, the model is more constant. 

We will now turn to the construction of confidence intervals for our model. First we will use nonparametric bootstrap to create a combinant based confidence interval for the model as described on page 220. The code used for the pair sampling can be seen below

```{r warning = FALSE, echo = TRUE, message = FALSE, fig.width = 10, fig.height = 6, fig.align = 'center'}
# Pair sampling
B <- 1000
set.seed(170)
n <- nrow(rain.data1)
boot_pred <- matrix(nrow = n, ncol = B)

for(b in 1:B){
  boot_samp <- sample(n, replace = TRUE)
  boot_mod <- glm(Rain ~ ns(SOI, df = 2), 
                  data = rain.data1[boot_samp, ], 
                  family = Gamma("log"))
  boot_pred[,b] <- predict(boot_mod, newdata = rain.data1, type = "response")
}

CIs <- matrix(nrow = n, ncol = 2)
for(i in 1:n){
  CIs[i,] <- 2*pred2[i] - quantile(boot_pred[i,], probs = c(0.975, 0.025), na.rm = TRUE)
}

p1 <- qplot(rain.data1$SOI, rain.data1$Rain) + 
  geom_line(aes(y = pred2), color = "black") +
  geom_ribbon(aes(ymin = CIs[,2], ymax = CIs[,1]), alpha = 0.3) +
  xlab("SOI") +
  ylab("Rain") +
  ggtitle("Pair sampling comb. based CI")
```

We will further construct a confidence interval of the form

$$
\hat{f} \pm 1.96 \hat{\text{se}} 
$$
We will use the analytical estimates of the standard errors of the model predictions. Note that this interval will be symmetric around the point estimate, and that the interval is based on distributional assumptions regarding the $Z$-score. For this reason it could be interesting to compare this approach with the bootstrap approach.

```{r warning = FALSE, echo = TRUE, message = FALSE, fig.width = 10, fig.height = 6, fig.align = 'center'}
SE_an <- predict(mod_final, newdata = rain.data1, type = "response", se.fit = TRUE)$se.fit
CIs2 <- cbind(pred2 - SE_an*qnorm(0.975), pred2 + SE_an*qnorm(0.975))

p2 <- qplot(rain.data1$SOI, rain.data1$Rain) + 
  geom_line(aes(y = pred2), color = "black") +
  geom_ribbon(aes(ymin = CIs2[,2], ymax = CIs2[,1]), alpha = 0.3) +
  xlab("SOI") +
  ylab("Rain") +
  ggtitle("Analytical SE CI")
```

We will now compare the two confidence intervals.

```{r warning = FALSE, echo = FALSE, message = FALSE, fig.width = 10, fig.height = 6, fig.align = 'center'}
grid.arrange(p1, p2, nrow = 1)
```

We see that the two confidence interval are narrow for the `SOI` values where we have many observations and wide for the `SOI` values for which we have few observations. This is to be expected. A thing to point out is the asymmetry that is present in the bootstrap combinant based confidence interval, indicating a certain asymmetry of the distribution of the model predictions. Note also, that the bootstrap confidence interval covers negative values, which is of course a poor prediction of rainfall. Apart from this, the two confidence intervals resemble each other quite a lot.

## Conclusion
The analysis carried out in this project suggests that both SOI phase and SOI are significant predictors of the rainfall at Eromanga in Queensland, Australia during the month of July. 

In the first part of the analysis we fitted a Tweedie model with `Phase` as predictor to conclude that `Phase` was a significant predictor of `Rain`.

In the second part of the analysis, we chose to model the data conditionally on having rained to provide a different perspective and avoid repeating previous steps of the analysis. Given the persistent right skew in the data and the AIC profile-likelihood estimate of $k = 2.198$, we selected a Gamma model which corresponds to a Tweedie model with $k = 2$. We chose the log link to keep predictions within the support of the distribution and because this model performed best in terms of training error compared to a model with the inverse-link. We decided not to include further predictors as they appeared to be insignificant. We finally included a natural cubic spline with 2 degrees of freedom since this model had the smallest generalization error compared to models fitted on natural cubic splines with degrees of freedom ranging from 1 to 6. We were finally able to conclude that `SOI` is a significant predictor of `Rain` and use the model to predict rainfall at different `SOI` values.

Although the models from part one and part two of the analysis are not directly comparable, their predictions appeared to be within the same range. The Tweedie model from part one predicted large amounts of rainfall (point estimate of $33.95$) when the SOI phase is consistently positive and small amounts of rainfall (point estimate of $2.34$) when SOI phase is rapidly falling. The Gamma model from part two predicted large amounts of rainfall when SOI is high (point estimate of $35.57$ when SOI is $20$) and small amounts of rainfall when SOI is low (point estimate of $4.01$ when SOI is $-20$). This suggest that `Rain` is increasing as a function of `SOI` and that a consistently positive or rapidly rising SOI phase leads to larger amounts of rain than consistently negative or rapidly falling SOI phases.

For future work it would be interesting to fit a Tweedie model with `SOI` as predictor to use the zero rainfall observations and compare the findings of this model with the findings of the Tweedie model using `Phase` as predictor. It would also be interesting to fit a Tweedie model conditionally on having rained using the estimate of $k$ determined by the AIC profile likelihood and compare this model to the Gamma model we fitted in the analysis. Finally, it would be of interest to provide prediction intervals for the model predictions to give a more complete picture of the uncertainty of the predictions.


