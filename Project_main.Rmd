---
title: "Regression Aflevering"
author: "Chrsitian Hejstvig Larsen and Michaela Lukacova"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries}
library(GLMsData)     # Project specific package
library(statmod)      # Project specific package
library(tweedie)      # Project specific package
library(mgcv)         # Project specific package

library(dplyr)        # For data manipulation
library(reshape2)     # For data manipulation
library(tidyverse)    # For data manipulation

library(splines)      # For splines

library(ggplot2)
theme_set(theme_bw())
library(gridExtra)    # For arranging plots
library(grid)         # For arranging plots

library(Hmisc)        # For multiple imputation
library(mice)         # For multiple imputation
library(sjmisc)       # For combining multiple imputation

library(pander)       # For printing tables
library(knitr)        # For printing tables
```

# Rainfall and Southern Oscillation Index (SOI) data

```{r data}
Rain.data = read.table("RaindataEromanga.txt", header = TRUE, colClasses = c("integer", "numeric", "integer", "numeric", "factor"))
head(Rain.data)
summary(Rain.data)
```

## Missing data

From the summary we see that only the Rain variable has missing values. The missingness is relatively sparse (7.5 % of the response values). We examine data in order to discover any pattern. 

```{r}
Rain.data[which(is.na(Rain.data$Rain)),] 
```
It seems as if the missingness occurs for consecutive years. Other than that, there is no clear pattern. We have no idea of knowing why data is missing. It could be missing completely at random, at random or not at random. Quite unlikely somebody could have deleted all the large observations of rain fall, this would be missing not at random. Or, more likely, it could simply be that some years the data was not recorded for some reason. This would be missing at random. The fact that the missingness happens for years in a row supports the hypothesis of missing at random rather than completely at random. 

We decide to do multiple imputations in order to account for the missingness. This procedure assumes missing at random. We perform multiple imputations in an attempt not to bias the results of our analysis. 

The multiple imputations are done with help of the aregImpute algorithm.  mice constructs multiple datasets with imputed values for the missing data and then combines the results. There is a certian randomness in each imputation so as not to artificially inflate the information in the data. Using multiple imputations gets rid of the arbitrariness of the imputation. We use the default method, which is predictive mean matching. We use 50 imputations, 50 iterations and a seed of 500.

```{r}
Rain.data.impute <- mice(Rain.data, method = "pmm", 
                         m = 5, seed = 10102024,  
                         printFlag = FALSE)

grouped_imputatations <- list()
for (i in 1:5) {
  grouped_imputatations[[i]] <- complete(Rain.data.impute, i) %>% group_by(Phase) %>% 
    summarise(meanY = mean(Rain), varY = var(Rain))
}

complete_data <- Rain.data %>% 
  mutate(Rain = merge_imputations(Rain.data, Rain.data.impute)$Rain)
```


## EDA

Marginal distributions

```{r}
tmp <- lapply(names(complete_data), function(x)
  ggplot(data = complete_data[, x, drop = FALSE])+
    aes_string(x) + xlab(x) + ylab(""))

gd <- geom_density(adjust = 2, fill = gray(0.5))
gb <- geom_bar(fill = gray(0.5))

grid.arrange(
  tmp[[1]] + gd,
  tmp[[2]] + gd,
  tmp[[3]] + gb,
  tmp[[4]] + gd
)
```
Rain is quite skewed, and there are not many phase 3 observations. 

### SOI and Phase

SOI is a numeric variable, it is the standardized difference between the air pressures at Darwin and Haiti, related to el niÃ±o. Phase is a categorical variable, with 5 levels, and is in some way a discretion of the variable SOI. The 5 levels of Phase relate to the sign of SOI but also the direction of this random variable, i.e. whether it is increasing, decreasing or constant. Thus SOI and Phase are related, but both contain information that the other does not. SOI is numeric, and thus more precise, while Phase contains information regarding direction. We investigate how the two variables relate to each other

```{r}
summary(lm(SOI ~ Phase, Rain.data))
```
The adjusted $R^2$ is quite high and the coefficients are highly significant indicating that the Phase variable is a good predictor of SOI. Thus the two variables are very correlated and we must be carefull to include both in our analysis. 

## Analysis using SOI phase

## Analysis using SOI directly

Model the rain fall as a function of SOI. Since the Tweedie model did not fit data very well

### Model rainfall conditionally on having rained

To model the rainfall conditionally on having rained, we first filter out the observations where the rain fall is zero. 

```{r}
rain.data1 <- complete_data %>%
  filter(Rain != 0)
```

We look at the distribution of the rain fall. 

```{r}
ggplot(data = rain.data1, aes(x = Rain)) +
  geom_density(fill = gray(0.5)) +
  xlab("Rain") +
  ylab("Density") +
  ggtitle("Rain fall distribution")
```
The distribution is still very right skewed. This motivates us to consider the Gamma exponential dispersion model, which is typically used to fit positive continous right skewed data. Consider now the rain fall as a function of the SOI. 

```{r warning = FALSE, message = FALSE}
# Plot rain against year using ggplot
ggplot(data = rain.data1, aes(x = SOI, y = Rain)) +
  geom_point() +
  geom_smooth() +
  xlab("SOI") +
  ylab("Rain") + 
  ggtitle("Rain against SOI")

```
There seems to be a positive relationship between the SOI and the rain fall. The plot also indicates a possibility of non linear trends. The Gamma exponential dispersion model has the variance structure $\mathcal{V} (\mu) = \mu^2 $, which also seems like a good fit from the plot above. 

If we choose the log link function, we fit the log of the mean of the response variables as a linear combination of the predictors. Other options include the identity link and the canonical link, which is the inverse function. A problem we have encountered when using the identity link is that for some parameters the model produces negative predictions, which is not possible for the rain fall, and furthermore causes convergence issues. For this reason the identity link is disregarded. We fit two models: one with log link and one using the canonical link. 

```{r}
glm_gamma_log <- glm(Rain ~ SOI, data = rain.data1, family = Gamma("log"))
glm_gamma_inv <- glm(Rain ~ SOI, data = rain.data1, family = Gamma)
```

Below we calculate the training error based on the squared deviance loss function. 

```{r}
tibble(
  "Link function" = c("Log", "Inverse"),
  "Training error" = c(mean(residuals(glm_gamma_log, type = "deviance")^2), 
                       mean(residuals(glm_gamma_inv, type = "deviance")^2))
) %>% pander()
```
The log link function seems to be the best fit in terms of training error. We plot the model fits:

```{r}
log_pred <- predict(glm_gamma_log, newdata = rain.data1, type = "response")
inv_pred <- predict(glm_gamma_inv, newdata = rain.data1, type = "response")

grid.arrange(
  qplot(rain.data1$SOI, rain.data1$Rain) + 
    geom_line(aes(y = log_pred), color = "red") +
    xlab("SOI") +
    ylab("Rain")+
    ggtitle("Log link function"),
   qplot(rain.data1$SOI, rain.data1$Rain) + 
    geom_line(aes(y = inv_pred), color = "red") +
    xlab("SOI") +
    ylab("Rain") +
    ggtitle("Inverse link function"),
  nrow = 1
)
```
The Gamma model with alog link function seems to fit data reasonably well. We choose to proceed with the Gamma model and the log link function. 

### Should we include additional predictors?

```{r}
add1(glm_gamma_log, Rain ~ SOI + Phase + Year, test = "LRT")
```
According to the LRT test there is not evidence in data that suggest that additional predictors should be added to the model. (The LRT test statistic is the comuputed as the difference in deviances between the nested models.)

### Explore possible inclusion of nonlinear effects

We consider to include a non-linear effect of SOI. We consider a natural cubic spline with 2,3,4 degrees of freedom:

```{r}
form1 <- Rain ~ SOI
form2 <- Rain ~ ns(SOI, df = 2)
form3 <- Rain ~ ns(SOI, df = 3)
form4 <- Rain ~ ns(SOI, df = 4)
form5 <- Rain ~ ns(SOI, df = 5)
form6 <- Rain ~ ns(SOI, df = 6)

glm1 <- glm(form1, data = rain.data1, family = Gamma("log"))
glm2 <- glm(form2, data = rain.data1, family = Gamma("log"))
glm3 <- glm(form3, data = rain.data1, family = Gamma("log"))
glm4 <- glm(form4, data = rain.data1, family = Gamma("log"))
glm5 <- glm(form5, data = rain.data1, family = Gamma("log"))
glm6 <- glm(form6, data = rain.data1, family = Gamma("log"))
```

The model fits: 

```{r}
pred1 <- predict(glm1, newdata = rain.data1, type = "response")
pred2 <- predict(glm2, newdata = rain.data1, type = "response")
pred3 <- predict(glm3, newdata = rain.data1, type = "response")
pred4 <- predict(glm4, newdata = rain.data1, type = "response")
pred5 <- predict(glm5, newdata = rain.data1, type = "response")
pred6 <- predict(glm6, newdata = rain.data1, type = "response")

grid.arrange(
  qplot(rain.data1$SOI, rain.data1$Rain) + 
    geom_line(aes(y = pred1), color = "red") +
    xlab("SOI") +
    ylab("Rain")+
    ggtitle("Df = 1"),
   qplot(rain.data1$SOI, rain.data1$Rain) + 
    geom_line(aes(y = pred2), color = "red") +
    xlab("SOI") +
    ylab("Rain") +
    ggtitle("Df = 2"),
   qplot(rain.data1$SOI, rain.data1$Rain) + 
    geom_line(aes(y = pred3), color = "red") +
    xlab("SOI") +
    ylab("Rain") +
    ggtitle("Df = 3"),
  qplot(rain.data1$SOI, rain.data1$Rain) + 
    geom_line(aes(y = pred4), color = "red") +
    xlab("SOI") +
    ylab("Rain") +
    ggtitle("Df = 4"),   
  qplot(rain.data1$SOI, rain.data1$Rain) + 
    geom_line(aes(y = pred5), color = "red") +
    xlab("SOI") +
    ylab("Rain") +
    ggtitle("Df = 5"),
  qplot(rain.data1$SOI, rain.data1$Rain) + 
    geom_line(aes(y = pred6), color = "red") +
    xlab("SOI") +
    ylab("Rain") +
    ggtitle("Df = 6"),
  nrow = 2
)
```

Adding more degrees of freedom to the natural cubic splines adds flexibility to the model, allowing it to fit data better. This comes at the expense of potentially overfitting. The model fitted with 6 degress of freedom is quite likely overfitting data. But in order to better assess which model is the best in terms of prediction, we do cross-validation to compare the models. We first define the error function. We use the deviance loss function.

```{r}
# Error function
dev_loss <- function(Y, muhat) 2 * (log(muhat / Y) + Y / muhat - 1)
```

The cross validation function:

```{r}
cv <- function(data, form, B = 1, k = 8, my_family, error_func){
  n <- nrow (data)
  PEcv <- vector("list", B)
  tmp <- numeric(n)
  for (b in 1: B){
    ## Generating the random division into groups
    group <- sample(rep(1:k, length.out = n))
    for (i in 1:k){
      modelcv <- glm(form, data = data[group != i, ], family = my_family)
      muhat <- predict(modelcv, newdata = data[group == i, ], type = "response")
      # !!! change input of error function !!!
      tmp[group == i] <- error_func(data$Rain[group == i],  muhat)
    }
    PEcv[[b]] <- tmp
  }
  mean(unlist(PEcv))
}
```

Since the data set is quite small, we perform LOOCV. Since this is a non random procedure $B$ is set to $1$.

```{r}
cv(data = rain.data1, form = form1, B = 1, k = nrow(rain.data1), 
   my_family = Gamma("log"), 
   error_func = dev_loss)

cv(data = rain.data1, form = form2, B = 1, k = nrow(rain.data1), 
   my_family = Gamma("log"), 
   error_func = dev_loss)

cv(data = rain.data1, form = form3, B = 1, k = nrow(rain.data1), 
   my_family = Gamma("log"), 
   error_func = dev_loss)

cv(data = rain.data1, form = form4, B = 1, k = nrow(rain.data1), 
   my_family = Gamma("log"), 
   error_func = dev_loss)

cv(data = rain.data1, form = form5, B = 1, k = nrow(rain.data1), 
   my_family = Gamma("log"), 
   error_func = dev_loss)

cv(data = rain.data1, form = form6, B = 1, k = nrow(rain.data1), 
   my_family = Gamma("log"), 
   error_func = dev_loss)
```
We proceed with the model with 2 degrees of freedom as it has the smallest cross validation error. But note that the model with 1 degree of freedom is very close to performind just as well in terms of generalization error. 

### Model Diagnostics

We do model diagnostics for the chosen model. 

```{r}
mod_final <- glm(form2, data = rain.data1, family = Gamma("log"))

glm_gamma.obj <- transform(
  rain.data1,
  .fitted = predict(mod_final, type = "response"),
  .deviance = residuals(mod_final, type = "deviance"),
  .pearson = residuals(mod_final, type = "pearson")
)

grid.arrange(
  qplot(.fitted, .deviance, data = glm_gamma.obj) + geom_smooth(),
  qplot(.fitted, .pearson, data = glm_gamma.obj) + geom_smooth(),
  qplot(.fitted, sqrt(abs(.pearson)), data = glm_gamma.obj) + geom_smooth(),
  ncol=3
)
```

The model diagnostics look quite good. There is no clear evidence against the model assumptions. We evaluate these plots via bootstrapping. We compare the residuals with simulated residuals under the null hypothesis that our model is correct.

```{r}
set.seed(1986234)
glm_list <- list()
for(i in 1:6){
  sim_data <- simulate(mod_final)[,1]
  glm_list[[i]] <- glm(sim_data ~ ns(rain.data1$SOI, df = 2),
                       family = Gamma(link = "log"))
}

grid.arrange(
  qplot(fitted(glm_list[[1]]), residuals(glm_list[[1]], type = "deviance")) + 
    xlab("Fitted values") +
    ylab("Deviance residuals") +
    geom_smooth(),
  qplot(fitted(glm_list[[2]]), residuals(glm_list[[2]], type = "deviance")) + 
    xlab("Fitted values") +
    ylab("Deviance residuals") +
    geom_smooth(),
  qplot(fitted(glm_list[[3]]), residuals(glm_list[[3]], type = "deviance")) + 
    xlab("Fitted values") +
    ylab("Deviance residuals") +
    geom_smooth(),
  qplot(fitted(glm_list[[4]]), residuals(glm_list[[4]], type = "deviance")) + 
    xlab("Fitted values") +
    ylab("Deviance residuals") +
    geom_smooth(),
   qplot(fitted(glm_list[[5]]), residuals(glm_list[[5]], type = "deviance")) + 
    xlab("Fitted values") +
    ylab("Deviance residuals") +
    geom_smooth(),
  qplot(fitted(glm_list[[6]]), residuals(glm_list[[6]], type = "deviance")) + 
    xlab("Fitted values") +
    ylab("Deviance residuals") +
    geom_smooth(),
  ncol = 3
)

grid.arrange(
  qplot(fitted(glm_list[[1]]), residuals(glm_list[[1]], type = "pearson")) + 
    xlab("Fitted values") +
    ylab("Pearson residuals") +
    geom_smooth(),
  qplot(fitted(glm_list[[2]]), residuals(glm_list[[2]], type = "pearson")) + 
    xlab("Fitted values") +
    ylab("Pearson residuals") +
    geom_smooth(),
  qplot(fitted(glm_list[[3]]), residuals(glm_list[[3]], type = "pearson")) + 
    xlab("Fitted values") +
    ylab("Pearson residuals") +
    geom_smooth(),
  qplot(fitted(glm_list[[4]]), residuals(glm_list[[4]], type = "pearson")) + 
    xlab("Fitted values") +
    ylab("Pearson residuals") +
    geom_smooth(),
   qplot(fitted(glm_list[[5]]), residuals(glm_list[[5]], type = "pearson")) + 
    xlab("Fitted values") +
    ylab("Pearson residuals") +
    geom_smooth(),
  qplot(fitted(glm_list[[6]]), residuals(glm_list[[6]], type = "pearson")) + 
    xlab("Fitted values") +
    ylab("Pearson residuals") +
    geom_smooth(),
  ncol = 3
)

```
The bootstrapped residual plots resemble the original residual plots sufficiently well. This indicates that the fitted model is correct.

### Report a final model

Final model fit

```{r}
pred2 <- predict(mod_final, newdata = rain.data1, type = "response")

qplot(rain.data1$SOI, rain.data1$Rain) + 
  geom_line(aes(y = pred2), color = "red") +
  xlab("SOI") +
  ylab("Rain") +
  ggtitle("Rain ~ ns(SOI, df = 2)")
```

### Interpretation of the results.

```{r}
summary(mod_final)
```



